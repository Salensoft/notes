
* grade
  exam 50
  assignment 10
  in-class quiz 7
  discussions 12
  experiment report 9 yanshou 12
* Overview
** background
*** purpose
*** content and schedule
* Chap1 Introduction
** what operating systems do
** computer-system organization
   + *device driver* for each device controller
   + *interrupt-request line*
     *interrupt-handler routine*
** computer-system architecture
   + four components
     1. Hardware
        CPU, memory, I/O system
     2. OS
     3. system and application program
     4. user
   + trap ::
     + either an *error* or a *user request*
    +
   + Direct Memory access structure ::
     + without CPU intervention
     + only one interrupt is generated *per block*
   + *multiprocessor* environment must provide *cache coherency* in hardware s.t.
     all CPUs have the most recent value in their cache.
   + *non-uniform memory access*
** operating system structure
   + difference ::
     + different purpose
       1. doesn't guarantee the interactivity, some may not be excute forever.
          system's perspective
       2. from user's perspective
     +
** operating system operations
   I/O devices and the CPU can execute concurrently
   each device controller has a *local buffer*
   I/O is from the device to local buufer of controller
   + *multiprogramming* CPU utilization
     + *job scheduling*
   + *timesharing(multitasking)* interactivity
     + CPU switches jobs so frequently that users can interact with each job
     + *response time* should be <1 seconds
   + interrupt driven by hardware
   + dual-mode ::
     + allows OS to protect itself and other system components
     + *user mode* and *kernel mode*
     + *mode bit*
       + some instructions designated as *privileged*
   +
** process management
   + A _process_ is a program in execution.
   + process needs resources to accomplish its task
   + multi-threaded process has one *program counter* per thread.
** memory management
   + memory management determines what is in memory when
     + optimizing CPU utilization and computer response to users.
   + memory management activities.
     + keeping track of which parts of memory are currently being
       used and by whom
     + deciding which processes and data to move into and out of memory
     + allocating and deallocating memory space
** storage management
* Chap2 operating-system structure
** operating system services
   + UI user interface
   + program execution
   + I/O operations
   + file-system manipulation
   + communications
   + error detection
   + resource allocation
   + accounting
   + protection and security
** UI
   + command-line interface CLI
   + GUI
** system call
   + programming interface to the services provided by the OS
     Application program interface API
   + pass parameters
     + simplest: pass the parameters in registers
     + parameter placed onto the stack
** types of system calls
   + process control
   + file management
   + device management
   + information maintenance
   + communications
** system programs
** operating system design and implementation
   + *Policy*: what will be done
     *Mechanism*: How to do it
** operating system structure
   + microkernel system structure
     + moves as much from the kernel into "user" space
     + communications takes place between *user modules* using *message passing*
     + benefits
       easier to extend to a microkernel
       easier to port the operating system to new architecture
       reliable
       secure
     + detriments
       performance overhead
   + modules
     *kernel modules*
** virtual machine
   + a virtual machine takes the layered approach to its logical conclusion
     it treats hardware and the operating system kernel as though they were all hardware
* Chap3 Process
** Process concept
   + a *process* is a program in execution
   + process state ::
     + new
       running
       waiting
       ready
       terminated
   + *Process control block(PCB)*
** Process scheduling
   + *process scheduler*
   + job queue
     ready queue
     device queue
** Operations on Processes
   + *process identifie(pid)*
** Interprocess Communication(IPC)
   + mechanism for processes to communicate and to synchronize their actions
   + two models *message passing* and *shared memory*
   + If P and Q wish to communicate, they need to
     + establish a *communication link* between them
     + exchange messages via *send/receive*
   + Direct communicate ::
     + links are eatablished *automatically*
     + a link is associated with *exactly one pair* of communicating processes.
     + between each pair there exists *exactly one link*
   + indirect communication ::
     + mesages are directed and received from *mailboxes(ports)*
     + a link may be associated with *many processes*
     + each pair of processes may share *several communication links*
   + synchronization ::
     + message passing may be either blocking or non-blocking
     + *blocking* is *synchronous*
       + *blocking send* has the sender block until the message is received.
       + *blocking receive* has the receiver block until a message is available
     + *non-blocking* is *asynchronous*
       + *non-blocking* send has the sender send the message and continue
       + *non-blocking* receive has the receiver receive a valid message or null
* Chap4 Threads
** Multithreading model
   + many(user thread)-to-one(kernel thread) model
     only one thread can access the kernel at a time. They may be blocked.
     simpler
   + one-to-one model
     more concurrency thant the many-to-one
     large number of kernel threads may burden the performance of a system
   + many-to-many(≤) model
     can create many user threads and corresponding kernel threads can run in parallel
   + two-level model
     allow many-to-many and one-to-one
** Implicit threading
*** thread pools
    creates a number of threads at start-up and place them into a pool
*** fork join
    
* Chap5 CPU scheduling
** Basic concepts
   + Maximum CPU utilization obtained with multiprogramming
   + *CPU-I/O burst cycle* - Process execution consists of a _cycle_ of CPU execution and I/O wait
     Process begins with a *CPU burst*, which is followed by an *I/O burst*
** CPU scheduler
   short-term scheduler
   select from among the processes in memory that are ready to execute, and allocates the CPU
   to two of them
   1. switches from running to waiting
   2. switches from running to ready
   3. switches from waiting to ready
   4. terminates
   scheduling under 1 and 4 is *nonpreemptive*
   all other scheduling is *preemptive* (kernel can take back the CPU)
** Scheduling criteria
   + CPU utilization
   + throughput吞吐率: the number of processes that are completed per time unit
   + turnaround time周转时间
   + waiting time
   + response time
** Scheduling algorithm
*** First-come, First-served scheduling(FCFS)
    + Gantt chart :: bar chart that illustrates a project schedule
    + Example
      | Process | Burst time |
      | P₁      |         24 |
      | P₂      |          3 |
      | P₃      |          3 |

      | ___P₁________ | __P₂__ | __P₃__ |
      0               24       27       30
      | _____P₁_____ | _P₂_ | __P₃__ | _P₄_ | ___P₅___ |
      0              10     11       13     14         19
      waiting time: P₁=24, P₂=3, P₃=3
      CPU utilization: 100%
      waiting time: P₁=0, P₂=24, P₃=27
      turnaround time: P₁=24, P₂=27, P₃=30

      *convoy effect*: all the other processes wait for the one big process to get off the CPU
*** Shortes-Job-First scheduling(SJF)
    + example non-preemptive SJF
      | Process | Arrival time | burst time |
      | P₁      |            0 |          7 |
      | P₂      |            2 |          4 |
      | P₃      |            4 |          1 |
      | P₄      |            5 |          4 |

      |_____P₁_____|__P₃__|___P₂___|___P₄___|
      0            7      8        12       16

      | _P₂_ | _P₄_ | __P₃__ | ___P₅___ | _____P₁_____ |
      0      1      2        4          9              19
    + preemptive SJF
      |___P₁___|___P₂___|__P₃__|___P₂___|____P₄____|_____P₁_____|
      0        2        4      5        7          11           16
      average waiting time = (9 + 1 + 0 + 2) / 4 = 3
    + SJF is optimal - gives minimum average waiting time
    + can be done by using the length of previous CPU bursts using exponential averaging
      1. tₙ=actual length of n-th CPU burst
      2. τₙ₊₁=predicted value for the next CPUs
      3. α, 0≤α≤1
      4. τₙ₊₁=αtₙ+(1-α)τₙ
*** Priority scheduling
    + each process has a priority number
    + CPU is allocated to the process with the highest priority
    + starvation - low priority processes may never be execute
    + solution - as time progresses increase the priority
      | _P₂_ | ____P₅____ | _____P₁_____ | __P₃__ | _P₄_ |
      0      1            6              16       18     19
*** Round-Robin scheduling (RR)
    + each process get a small unit of CPU time(time quantum), usually 10-100 milliseconds.
      After this time has elapsed, the process is preempted and added to the end of the ready
      queue
    + example
      | Process | Burst time |
      | P₁      |         53 |
      | P₂      |         17 |
      | P₃      |         68 |
      | p₄      |         24 |

      | __P₁__ | __P₂__ | __P₃__ | __P₄__ | ..


      | _P₁_ | _P₂_ | _P₃_ | _P₄_ | _P₅_ | _P₁_ | _P₃_ | _P₅_ | _P₁_ | _P₅_ | _P₁_ | _P₅_ | _P₁_ | _P₅_ | ___P₁___ |
      0      1      2      3      4      5      6      7      8      9      10     11     12     13     14         19
*** Multilevel queue
    + several queue
      each queue has its own schduling algorithm
** Multi-processor scheduling
   + *asymmetirc multiprocessing*:
     only one processor accesses the system data structures, alleviating the need for
     data sharing; others execute only user code
   + *symmetric multiprocessing(SMP)*
     each processor is self-scheduling. Multiple processors might access and update a
     common data structure
** Real-time scheduling
   + *Hard real-time systems*:
     required to complete a critical task within a guaranteed amout of time
   + *Soft real-time computing*:
     requires that critical processes receive priority over less fortunate ones
** Thread scheduling
   kernel-level threads are managed by a thread library, and user-level threads by thread library
   to run on a CPU, user-level threads must ultimately be mapped to an associated kernel-level
   thread
   lightweighted process(LWP)
*** contention scope
    + Process-contention scope(PCS) ::
      + the thread library schedules user-level threads to run on an available LWP
    + System-contention scope(SCS) ::
      + kernel decides which kernel-level thread to schedule onto a CPU
    + 
*** wef
  + *local scheduling*:
    how the threads library decides which thread to put onto an available LWP
  + *global scheduling*:
    how the kernel decides which kernel thread to run next
* Chap6 Process synchronization
** Background
   + concurrent access to shared data may result in data inconsistency
   + maintaining data consistency requires mechanisms to ensure the *orderly* execution
     of cooperating processes
   + Suppose that we wanted to provide a solution to the consumer-producer problem
     that fills all the buffers. We can do so by having an integer count that keeps
     track of the number of full buffers.  Initially, count is set to 0. It is incremented
     by the producer after it produces a new buffer and is decremented by the consumer
     after it consumes a buffer.
** Critical-section problem
#+BEGIN_SRC c
  do {
    ENTRY SECION;
    Critical section;
    EXIT SECTION;
    Remainder section
  }while(true)
#+END_SRC
   + solution
     + Mutual exclusion ::
       + if process Pᵢ is executing in its critical section, then no other processes can be
         executing in their critical sections
     + Progress ::
       + If no process is executing in its critical section and there exist some
         processes that wish to enter their critical section, then the selection of the
         processes that will enter the critical section next cannot be postponed indefinitely
     + bounded waiting ::
       + A bound must exist on the number of times that other processes are allowed to
         enter their critical sections after a process has made a request to enter its
         critical section and before that request is granted
       + Assume that each process executes at a nonzero speed
** Peterson's solution
   + two process solution
   + assume LOAD and STORE instructions are atomic
   + two processes share two variables
     int *turn*;
     boolean *flag[2]*
   + the variable *turn* indicates whose turn it is to enter the critical section
   + *flag* array is used to indicate if a process is ready to enter the critical section
** Synchronization Hardware
   + uniprocessors
*** *atomic hardware instructions*
   Atomic = non-interruptable
   + TestAndSet instruction
#+BEGIN_SRC c
  boolean TestAndSet(boolean *target) {
    boolean rv = *target;
    *target = TRUE;
    return rv;
  }
#+END_SRC
#+NAME: solution using TestAndset
#+BEGIN_SRC c
  while (true) {
  //first guy use TestAndSet would do things
    while ( TestAndSet (&lock ))
  // do nothing
  //    critical section
      lock = FALSE;
  //      remainder section
  }

#+END_SRC
   + Swap instruction
#+NAME: swap
#+BEGIN_SRC c
  void Swap (boolean *a, boolean *b)
  {
    boolean temp = *a;
    *a = *b;
    *b = temp:
  }
#+END_SRC
#+NAME: solution using swap
#+BEGIN_SRC c
  while (true){
    key = TRUE;
    while ( key == TRUE)
      Swap (&lock, &key );
    //    critical section
    lock = FALSE;
    //      remainder section
   }
#+END_SRC
** Semaphore
   + synchronization tool that is less complicated
   + semaphore *S* - integer variable
   + two *atomic* standard operations modify *S*: *wait()* and *signal()*
     originally *P()* and *V()*
   + 
#+BEGIN_SRC c
  wait (S) {
    while (S <= 0); // no-op
    S--;
  }

  signal (S) {
          S++;
  }
#+END_SRC
*** Usage as general synchronization tool
   + *counting* semaphore - integer value can rage over an unrestricted domain
   + *binary* semaphore - integer value can range only between 0 and 1
     also known as *mutex locks*
   + provide mutual exclusion
#+BEGIN_SRC c
  Semaphore S;
  wait(S);
  //Critical section
  signal(S);
#+END_SRC
   + P₁ has a statement S₁, P₂ has S₂
     P₁    S₁;
           Signal(S);

     P₂    Wait(S);
           S₂;
*** Semaphore implementation
    + must guarantee that no two processes can execute *wait()* and *signal()* on
      the same semaphore at the same time
    + implementation without busy waiting ::
      + with each semaphore there is an associated waiting queue.
        Each semaphore has two data items
        1. value
        2. pointer to a linked-list of PCBs.
      + two operations
        1. *block* - place the process invoking the operation on the appropiate waiting
           queue
        2. *wakeup* - remove one of processes in the waiting queue and place it in the
           ready queue
      + implementation
#+BEGIN_SRC c
  wait (S){
    value--;
    if (value < 0) {
      //add this process to waiting queue
      block();  }
  }

  Signal (S){
    value++;
    if (value <= 0) {
      //remove a process P from the waiting queue
      wakeup(P);  }
  }
#+END_SRC
*** problems with semaphores
    + correct use of semaphore operations
      1. signal(mutex) ... wait(mutex)
      2. wait(mutex) ... wait(mutex)
      3. omitting of wait(mutex) or signal(mutex)
** Deadlock and starvation
   + *Deadlock* – two or more processes are waiting indefinitely for an event that
     can be caused by only one of the waiting processes
   + *starvation* - indefinite blocking.  A process may never be removed from the
     semaphore queue in which it is suspended

** Classical problems of synchronization
*** Bounded-buffer problem
    + N buffers, each can hold one item
    + semahpore *mutex(binary)* initialized to 1 (exclusive)
    + semaphore *full* initialized to 0, counting full items
    + semaphore *empty* initialized to N, counting empty items
#+NAME: producer
#+BEGIN_SRC c
  while (true)  {
    //   produce an item
    wait (empty);
    wait (mutex);
    //  add the item to the  buffer
    signal (mutex);
    signal (full);
   }
#+END_SRC
#+NAME: consumer
#+BEGIN_SRC c
  while (true) {
    wait (full);
    wait (mutex);
    //  remove an item from  buffer
    signal (mutex);
    signal (empty);
    //  consume the removed item
   }
#+END_SRC
*** readers and writers problem
    + A data set is shared among a number of concurrent processes
      1. reader - only read the data set, they don't perform any updates
      2. writers - can both read and write
    + *problem* - allow multiple readers to read at the same time.
      only one single writer can access the shared data at the same time
    + shared data ::
      + data set
        semaphore *mutex* initialized to 1, to ensure mutual exclusion when
        *readcount* is updated
        semaphore *wrt* initialized to 1
        integer *readcount* initialized to 0
#+BEGIN_SRC c
  //writer
  while(true) {
    wait(wrt);
    //writing is performed
    signal(wrt);
   }
  //reader
  while (true) {
    wait (mutex) ;
    readcount ++ ;
    //if current reader is the only reader, it's the writer
    if (readcount == 1)  wait (wrt) ;
    signal (mutex);
    // reading is performed
    wait (mutex) ;
    readcount -- ;
    if (readcount  == 0)  signal (wrt) ;
    signal (mutex) ;
   }

#+END_SRC
*** dining-philosophers problem
    + shared data
      bowl of rice (data set)
      each needs 2 chopsticks to eat
      semaphore *chopstick[5]* initialized to 1
#+BEGIN_SRC c
  //philosopher i
  While (true)  { 
    wait ( chopstick[i] );
    wait ( chopStick[ (i + 1) % 5] );
    //  eat
    signal ( chopstick[i] );
    signal (chopstick[ (i + 1) % 5] );
    //  think
  }

#+END_SRC
** Monitor
   + a high-level abstraction that provides a convenient and effective *mechanism* for
     process synchronization
   + only *one* process may be active within the monitor at a time
     #+NAME: monitor
     #+BEGIN_SRC c
       monitor monitor-name
       {
         // shared variable declarations
         procedure P1 (…) { …. }
         …;
         procedure Pn (…) {……}
         Initialization code ( ….) { … }
         …
       }
     #+END_SRC
   + condition variables ::
     + _condition x,y_
     + two operations on a condition variables
       1. _x.wait()_ - a process that invokes the operation is suspended
       2. _x.signal()_ - resumes one of processes that invoked _x.wait()_
*** solution to dining philosophers
    #+BEGIN_SRC c
      monitor DP
      { 
        enum { THINKING; HUNGRY, EATING} state [5] ;
        condition self [5];  //philosopher i can delay herself when unable to get chopsticks

        void pickup (int i) { 
          state[i] = HUNGRY;
          test(i);
          if (state[i] != EATING) self [i].wait;
        }

        void putdown (int i) { 
          state[i] = THINKING;
          // test left and right neighbors
          test((i + 4) % 5);
          test((i + 1) % 5);
        }

        void test (int i) { 
          if ( (state[(i + 4) % 5] != EATING) &&
               (state[i] == HUNGRY) &&
               (state[(i + 1) % 5] != EATING) ) { 
            state[i] = EATING ;
            self[i].signal () ;
          }
        }

        initialization_code() { 
          for (int i = 0; i < 5; i++)
            state[i] = THINKING;
        }
      }

    #+END_SRC
    + each philosopher i invokes the operation _pickup()_ and _putdown()_ in the following
      sequence
      dp.pickup(i)
      EAT
      dp.putdown(i)
*** monitor implementation using semaphores
    + variables
      *semaphore mutex* (initially 1, entry protection, only one process)
      *semaphore next* (initially 0, signalling process may suspend themselves)
      *int next-count=0*
    + procedure *F*
      #+BEGIN_SRC c
        wait(mutex);
        …;			 
        body of F;
        …;
        if (next-count > 0)
          signal(next)
        else 
          signal(mutex);
      #+END_SRC
    + _x_
      #+BEGIN_SRC c
        semaphore x-sem; // (initially  = 0)
        int x-count = 0;
      #+END_SRC
    + _x.wait_
      #+BEGIN_SRC c
        x-count++; //number of process waiting
        if (next-count > 0)
          signal(next); //if someone has been waiting, wake her up because I'll be
                        //entering the waiting state
        else
          signal(mutex);//no one else waiting in the monitor. I'm going to block
        wait(x-sem);    //block myself
        x-count--;
      #+END_SRC
    + _x.signal_
      #+BEGIN_SRC c
        if (x-count > 0) {
          next-count++;
          signal(x-sem);//wait on the "next" semaphore
          wait(next);
          next-count--;
        }
      #+END_SRC
#+BEGIN_SRC c
  semaphore chairs = 0;
  semaphore customer = 0;
  int customers = 0;

  void fbarber() {
    if (customers == 0)
      wait(customer);
  }

  void fcustomer() {
 
  }
#+END_SRC
** Synchronization
*** pthreads synchronization
    + mutex example
      #+BEGIN_SRC c
                void reader_function ( void );
                void writer_function ( void ); 
                char buffer;
                int buffer_has_item=0;
                pthread_mutex_t mutex;
                struct timespec delay;
                void main ( void )
                {
                  pthread_t reader;
                  delay.tv_sec = 2;
                  delay.tv_nec = 0;
                  pthread_mutex_init (&mutex,NULL);
                  pthread_create(&reader, pthread_attr_default, (void *)&reader_function, NULL);
                  writer_function( );
                }
                void writer_function (void){
                  while(1){
                    pthread_mutex_lock (&mutex);
                    if (buffer_has_item==0){
                      buffer=make_new_item( );
                      buffer_has_item=1;}
                    pthread_mutex_unlock(&mutex);
                    pthread_delay_np(&delay);
                  }
                } 
        void reader_function(void){
          while(1){pthread_mutex_lock(&mutex);
            if(buffer_has_item==1){
              consume_item(buffer);
              buffer_has_item=0;}
            pthread_mutex_unlock(&mutex);
            pthread_delay_np(&delay);
          }
        }

      #+END_SRC
* Chap8 Deadlocks
** the deadlock problem
   + A set of blocked processes each holding a resource and waiting to acquire
     a resource held by another process in the set.
** system model
   + resource types R₁,R₂,...,Rₘ
   + each resource type Rᵢ has Wᵢ instances
   + each process utilizes a resource as follows
     1. request
     2. use
     3. release
** Deadlock characterization
   + mutual exclusion ::
     + only one process at a time can use a resource
   + hold and wait ::
     + a process holding at least one resource is waiting to acquire
       additional resources held by other process
   + no preemptive ::
     + a resource can be released only voluntarily by the process holding it, after
       that process has completed its task
   + circular wait ::
     + wait circular
*** resource-allocation graph
    + basic facts ::
      + if graph contains no cycle -> no dead lock
      + if graph contains a cycle ->
        + if only one instance per resource type, then deadlock
        + if several instances per resource type, possibility of deadlock
** Methods for handling deadlocks
   + ignore the problem and pretend that deadlocks never occur in the system;
     used by most operating systems, including UNIX
** Deadlock prevention
   + mutual exclusion ::
     + not required for sharable resources; must hold for nonsharable resources
   + Hold and wait ::
     + must guarantee that whenever a process requests a resource, it does not hold any
       other resources
     + Require process to request and be allocated all its resources before it begins
       execution, or allow process to request resources only when the process has none
       (release all current resources before requesting any additional ones).
     + Low resource utilization; starvation possible. (example: copy data from DVD drive
       to a disk file, sorts the file, then prints the results to a printer.)
   + no preemption ::
     + If a process that is holding some resources requests another resource that cannot
       be immediately allocated to it, then all resources currently being held are released.
     + Process will be restarted only when it can regain its old resources, as well as the
       new ones that it is requesting.
   + circular wait ::
     + impose a total ordering of all resource types, and require that each process
       requests resources in an increasing order of enumeration.
** Deadlock avoidance
   + require system having additional a priori information
   + simple: each process declares the *maximum number* of resources of each type that it may need.
   + The deadlock-avoidance algorithm *dynamically* examines the resource-allocation state to
     ensure that there can never be a circular-wait condition.
   + Resource-allocation *state* is defined by the number of available and allocated resources,
     and the maximum demands of the processes.
*** safe state
    + system is in *safe state* if there exists a sequence <P₁,P₂,...,Pₙ> of all the processes,
      is the systems s.t. for each Pᵢ, the resources that Pᵢ can still request can be
      satisfied by currently available resources + resources held by all the Pⱼ, j<i
    + facts
      safe state => no deadlocks
      unsafe state => possible deadlock
      avoidance => never enter an unsafe state
*** avoidance algorithms
    + single instance of a resource type : resource-allocation graph
    + multiple instances of a resource type : banker's algorithm
*** banker's algorithm
    + assumptions
      * multiple instances
      * each process must a priori claim maximum used
      * when a process requests a resource it may have to wait
      * when a process gets all its resources it must return them in a finite amout of time
    + example
      P₀ P₁ P₂ P₃ P₄
      3 source types: A(10), B(5), C(7)
      |    | Allocation | Max     | Available |
      |    | A  B  C    | A  B  C | A  B  C   |
      | P₀ | 0  1  0    | 7  5  3 | 3  3  2   |
      | P₁ | 2  0  0    | 3  2  2 |           |
      | P₂ | 3  0  2    | 9  0  2 |           |
      | P₃ | 2  1  1    | 2  2  2 |           |
      | P₄ | 0  0  2    | 4  3  3 |           |

      if each need MAX
      |    | Need  |
      |    | A B C |
      | P₀ | 7 4 3 |
      | P₁ | 1 2 2 |
      | P₂ | 6 0 0 |
      | P₃ | 0 1 1 |
      | P₄ | 4 3 1 |
      hence a sequence <P₁,P₃,P₄,P₂,P₀>
    + data structure
      1. *available*
      2. *max*
      3. *allocation*
      4. *need*
    + safety algorithm ::
      1. let *work* and *finish* be vectors of length m and n. Initialize:
         *work* = *available*
         *finish*[i] = false for i = 0,1,...,n-1
      2. Find an i s.t.
         1. *finish*[i] = false
         2. *need*[i] ≤ *work*
         3. if no such i, go to 4
      3. *work* = *work* + *allocation*
         *finish*[i] = true
      4. if *finish*[i] = true for all i, in a safe state
    + Resource-request algorithm for Pᵢ ::
      0. Request = request vector for process Pᵢ
** Deadlock detection
*** Single instance of each resource type
    + maintain wait-for graph
      + nodes are porcess
      + Pᵢ → Pⱼ if Pᵢ is waiting for Pⱼ
    + *Periodically* invoke an algorithm that searches for a cycle in the graph. If
      there is a cycle, there exists a deadlock
*** several instances of a resource type
    + data structure
      1. *available*: the number of available resources of each type
      2. *allocation*: the number of resources of each type currently allocated to each process.
      3. *request*: the current request  of each process
    + algorithm O(m×n²)
      1. *work* = *available*
         if *allocationᵢ* ≠ 0, then *finish*[i] = false, otherwise true
      2. if ∃i,
         *finish[i]* = false
         *request[i]* ≤ *work*
         if no i, go to 4
      3. *work* = *work* + *allocation*
         *finish[i]* = true
         go to 2
      4. if ∃i *finish[i]* = false, there is deadlock
** Recovery from deadlock: process termination
   + in which order should we choose to abort
     1. Priority of the process.
     2. How long process has computed, and how much longer to completion.
     3. Resources the process has used.
     4. Resources process needs to complete.
     5. How many processes will need to be terminated. 
     6. Is process interactive or batch?
   + resource reemption ::
     + selecting a victim - minimize cost
     + rollback - return to safe state, restart process for that state
     + starvation - same process may always be picked as victim, include number of
       rollback in cost factor
* Chap9 Main memory
** background
   + a pair of *base* and *limit* registers define the logical address space
   + binding of instructions and data to memory
     + address binding of instructions and data to memory addesses can happen at three
       different stages
       1. *compile time*:  If memory location known a priori, *absolute code* can be generated;
          must recompile code if starting location changes
       2. *load time*: must generate *relocatable code* if memory location is not known at compile
          time
       3. *execution time:* Binding delayed until run time if the process can be moved
          during its execution from one memory segment to another.  Need hardware support
          for address mapbinding delayed until run
*** logical and physical address space
    + *logical address* - generated by the CPU; also referred to as *virtual address*
    + *physical address* - address seen by the memory unit
    + *memory-management unit* - run-time mapping from virtual to physical address
    + *relocation register* - base register
    + logical and physical addresses are the *same* in compile-time and load-time address-binding
      schemes
    + logical address and physical addresses *differ* in execution-time address-bind schemes
    + memory management unit
      + hardware device that maps virtual to physical address
*** dynamic loading
    + routine is not loaded until it's called
    + better memory-space utilization; unused routine is never loaded
    + useful when large amounts of code are needed to handle infrequently occuring
      cases
    + done by application
*** dynamic linking
    + *dynamically linked libraries(DLLs)*
    + linking postoned until execution time
    + Small piece of code, stub, used to locate the appropriate memory-resident library routine
    + Stub replaces itself with the address of the routine, and executes the routine
    + OS needed to check if routine is in processes' memory address
    + system also known as *shared libraries*
    + easy to update
    + done by OS
** contiguous memory
   + In contiguous memory allocation, each process is contained in a single section of memory that
     is contiguous to the section containing the next process
   + main memory usually into two partitions
   + multiple-partition allocation ::
     + hole - block of available memory; holes of various size are scattered throughout memory
     + when a process arrives, it's allocated memory from a hold large enough to accommoda data
       + first-fit
         best-fit
         worst-fit
   + fragmentation ::
     + *external fragmentation* - total memory space exists to satisfy a request, but it's not
       contiguous
     + *internal fragmentation* - allocated memory may be slightly larger than requested memory
       this size difference is memory internal to a partition but not being used
     + reduce external fragmentation by *compaction*
       - shuffle memory contents to place all free memory together in one large block
       - compaction is possible _only_ if relocation is dynamic
** paging
   + logical address space of a process can be noncontiguous; process is allocated physical
     memory whenever the latter is available
   + divide physical memory into fixed-sized blocks called *frames*
   + divide logical memory into blocks of same size called *pages*
   + address generated by CPU is divided into
     1. *page number(p)* - used as an index into a page table
     2. *page offset(d)* -
   + free-frame list
*** hardware implementation of page table
     + page table is kept in main memory
     + *page-table base register(PTBR)* points to the page table
     + *page-table length register(PTLR)* indicates size of the page table
     + in this scheme every data/instruction access requires two memory accesses. one
       for the page table and one for the data/instruction
     + the two-memory-access problem can be solved by the use of special fast-lookup
       hardware cache called *associative memory* or *translation lookaside buffers(TLBs)*
     + effective access time ::
       + associative lookup = ε time unit
         assume memory cycle time is 1 microsecond
         hit ratio = α
         EAT = (1 + ε)α + (2 + ε)(1 - α)
             = 2 + ε - α
*** memory protection
    + *valid and invalid* bit attached to each entry in the page table
*** shared pages
    + *shared code* -
      - one copy of read-only code shared among processes
** structure of the paging
*** hierarchical paging
    + break up the logical address space into multiple page tables - to page the page table
    + because page table is not enough.
      two large memory
    + a simple technique is a two-level page table
    + [[./images/OperatingSystem/HierarchicalPagingTable.png]]
*** hashed page tables
    + common in address spaces > 32 bits
    + the virtual page number is hashed into a page table. This page table contains a
      chain of elements hashing to the same location
    + virtual page numbers are compared in this chain searching for a match. If a match
      is found, the corresponding physical frame is extracted
    + [[./images/OperatingSystem/HashedPageTable.png]]
    + could use TLB to accelarate
*** inverted page tables
    + one entry for each real page of memory
    + entry consists of the virtual address of the page stored in that real memory location
      with information about the process that owns that page
    + decreases memory needed to stored each page table, but increases time needed
      to search the table when a page references occurs
    + use hash table to limit the search to one - or at most a few - page-table entries
    + [[./images/OperatingSystem/InvertedPageTable.png]]
      
** swapping
   + A process can be swapped temporarily out of memory to a backing store, and then brought
     back into memory for continued execution
   + *Backing store* – fast disk large enough to accommodate copies of all memory images for
     all users; must provide direct access to these memory images
   + Roll out, roll in – swapping variant used for priority-based scheduling algorithms; 
     lower-priority process is swapped out so higher-priority process can be loaded and executed
** Segmentation
   + A program is a collection of *segments*. A *segment* is a logical unit such as
     main program
     procedure
     function
     method
     ...
   + [[./images/OperatingSystem/UserViewOfProgram.png]]
*** segmentation architecture
    + logical address consists of a two tuple
      <segment-number, offset>
    + *segment table* - maps two-dimemsionaphysical addresses; each table entry has
      + *base* - contains the starting physical address where the segments reside in memory
      + *limit* - specifies the length of the segment
    + *segment-table base register(STBR)* points to the segment table's location in memory
    + *segment-table length register(STLR)* indicates number of segments used by a program
    + protection
      * validation bit
      * read/write/execute privileges 
    + [[./images/OperatingSystem/SegmentationHardware.png]]
**** example: the intel pentium
     + support both segmentation and segmentation with paging(page the segmentation)
     + CPU generates logical address
       * logical address space divided into local and global partitions
       * LDT vs GDT table
         LDT(local descriptor/segment table)
         GDT(glocal ...)
     + logical address
       | selector                               | offset |
       | 16-bit                                 | 32-bit |
       | s(segmentation)  g(GDT)  p(protection) |        |
       | 13               1       2             |        |
     + linear address
       two level table
       P₁ outer page table
       p₂ inner page table
       | page number | page offset |
       | p₁   P₂     | d           |
       | 10   10     | 12          |
**** Linux
     + three-level paging in linux
* Chap10 virtual memory
** background
   + virtual memory can be implemented via
     demand paging
     demand segmentationx
   + shared memory is enabled
** demand paging
*** basic concept
    + Bring a page into memory only when it's needed
      * less I/O needed
      * less memory needed
      * faster response
      * more users
    + page is needed => reference to it
    + *lazy swapper* - never swaps a page into memory unless page will be needed
    + *valid-invalid bit*
      if valid-invalid bit in page table entry is *I* => *page fault* (a trap to the OS)
    + page fault ::
                    if there is a reference to a page
      1. operating system looks at *another table* (kept with PCB process control block) to decide
         invalid reference => abort
         Just not in memory                  \
      2. get empty frame                     |-Interrupt handler
      3. swap page into frame(slowest)       /
      4. reset tables                                        -\ 
      5. set validation bit = *v*                               |-bottom half (much more time)
      6. restart the instruction that caused the page fault  -/
    + [[./images/OperatingSystem/PageFault.png]]
    + Restart instruction
      * block move
      * auto increment/decrement location
*** Performance
    + page fault rate 0 ≤ p ≤ 1.0
      if p = 0, no page faults
    + Effective access time (EAT)
      EAT = (1 - p) × memory access + p(page fault overhead +
                                        swap page out + //swap the victim out
                                        swap page in +
                                        restart overhead)
    + example
      + memory access time = 200 ns
      + average page-fualt service time = 8 milliseconds
      + EAT = (1 - p) × 200 + p * (8 milliseconds)
            = 200 + p × 7,999,800
      + if  one access out of 1000 causes a page fault, then
        EAT = 8.2 microseconds
** copy-on-write
** page replacement
*** basic page replacement
    + use *modify(dirty) bit* - only modified pages are writtern to disk
    + algorithm
      1. find the location of the desired page on disk
      2. find a free frame
         - if there is free frame, use it
         - if not, use a page replacement algorithm to select a *victim* page
    + [[./images/OperatingSystem/PageReplacement.png]]
    + want lowest page-fault rate
    + evaluate algorithm by running it on a particular string of memory references and
      computing the number of page faults on that string
    + example
      1,2,3,4,1,2,5,1,2,3,4,5
*** FIFO page replacement
    + Belady's anomaly: more frames => more page faults
*** optimal algorithm
    + replace page that won't be used for longest period of time
*** LRU(Least recently used) page replacement
    + counter implementation
      + every page entry has a counter; every time page is referenced through
        this entry, copy the clock into the counter
      + when a page needs to be changed, look at the counters to determine which are
        to change
*** LRU approximation algorithms
    + reference bit
      * with each page associate a bit, initially = 0
      * when page is referenced bit set to 1
      * replace the one which is 0(if exists)
    + second chance
      * need reference
*** second-chance page replacement algorithm
*** counting algorithm
    + *LFU(least frequently used)*
      + n items in buffer
        n + 1 items
        then scan n + 1 items. This is awful in LRU
    + *MFU(most frequently used)*
** allocation of frames
*** minimum number of frames
    + each process needs *minimum* number of pages
*** allocation algorithm
    + *fixed allocation*
      + *equal allocation*
      + *proportional allocation*
        allocate according to the size
    + *priority allocation*
      + use proportional allocation scheme using priorities rather than size
*** global and local allocation
    + *global replacement* - process selects a replacement frame from the set of
      all frames; one process can take a frame from another
    + *local replacement* - each process selects from only its own set of allocated frames
** Thrashing
   + if a process doesn't have enough pages, the page-fault rate is very high
     this leads to
     * low CPU utilization 
     * queuing at paging device, the ready queue becomes empty
     * OS thinks that it needs to increase the degree of multiprogramming
     * another process added to the system
   + *Thrashing*: a process is busy swapping pages in and out
   + [[./images/OperatingSystem/Thrashing.png]]
*** Cause, Demand paging and thrashing
    + why does demand paging work
      Locality model
      Process migrates from one locality to another
      localities may overlap
    + why does thrashing occur
      Σsize of locality > total memory size
    + to *limit* the effect of thrashing
      local replacement algo cannot steal frames from other processes. But 
      queue in page device increases effective access time
    + to *prevent* thrashing:
      allocate memory to accommodate its locality
*** Working-set model
    + Δ ≡ working-set window ≡ a fixed number of page references
    + WSSᵢ(working set size of process Pᵢ) = total number of pages referenced in
      the most recent Δ
      + if Δ too small will not encompass entire locality
      + if Δ too large will encompass several localities
      + if Δ = ∞ => will encompass entire program
    + D = Σ WSSᵢ ≡ total demand frames for all processes in the system
    + if D > m => thrashing
    + Policy: if D > m then suspend one of the processes
    + approximate with interval timer + a reference bit
    + Example: Σ = 10,000
      * Timer interrupts after every 5000 time units
      * Keep in memory 2 bits for each page
      * Whenever a timer interrupts copy and sets the values of all reference bits to 0
      * If one of the bits in memory = 1  page in working se
** Memory mapped file
** Other issue
*** prepaging
    + to reduce the large number of page faults that occurs at process startup
    + prepage all or some of the pages a process will need, before they are referenced
*** page size
*** TLB reach
*** Program structure
* chap10 file-system interface
** file concept
   + *file system* 
     file naming
     where files are placed
     metadata
     access rules
   + types
     + data
       numeric
       character
       binary
     + program
       source
       object
       executable
   + file structure
   + file attribute
   + file operation
   + *open-file system*
     + open() system returns a pointer to an entry in the *open-file table*
     + per-process table
       current file pointer
       access rights
     + system-wide table
       open count
   + open file locking
** access method
*** sequential
*** direct\arbitrary\random access
** directory structure
   + a collection of nodes containing information about all files
   + [[./images/OperatingSystem/DirectoryStructure.png]]
** file-system mounting
   + 
** file sharing
** protection
* Chap 11 file system implementation
** file system structure
   + file structure
     - logical storage unit
     - collection of related information
   + file system resides on secondary storage
   + file system organized into layers
     [[./images/OperatingSystem/LayeredFileSystem.png]]
   + *file control block* - storage structure consisting of information about a file
     [[./images/OperatingSystem/FileControlBlock.png]]
   + *virtual file system*:
     * provide an object-oriented way of implementing file system
     * [[./images/OperatingSystem/VFS.png]]
     * allows the same system call interface (the API) to be used for different 
       types of file systems
     * Defines a network-wide unique structure called *vnode*
** file system implementation
   + data structure
*** Linear list
*** Hash table
*** Disk structure
    + Boot control block
    + volume control block
    + directory structure per file system
    + per-file FCB
*** in-memory structure
    + in-memory structures
    + directory cache
    + system-wide open-file table
    + per-process open-file table
    + [[./images/OperatingSystem/InMemoryFS.png]]
** directory implementation
** allocation methods
*** contiguous allocation
    + each file occupies a set of contiguous blocks on the disk
    + random access supported
    + wasteful of space
    + file cannot grow
    + mapping
      LA/512 - Q, R
      displacement into block = R
*** linked allocation
    + each file is a linked list of disk blocks: blocks may be scattered anywhere on
      the disk
    + *no random access*
    + mapping
      LA/511 - Q, R(final block)
      displacement into block = R + 1(have a pointer)
    + *file-allocation table(FAT)* - disk-space allocation used by MS-DOS and OS/2
      [[./images/OperatingSystem/FAT.png]]
*** indexed allocation
    + brings all pointers together into the *index block*
    + [[./images/OperatingSystem/IndexedAllocation.png]]
    + [[./images/OperatingSystem/LinkedScheme.png]]
*** inode
** free space management
   + bit vector
     requires extra space
   + linked list
   + grouping
   + counting
** Performance
   + a *page cache* caches pages rather than disk blocks using *virtual memory* techs
   + memory-mapped I/O uses a page cache
   + routine I/O through the file system uses the buffer(disk) cache
   + *unified buffer cache*
* Chap 12 Mass-storage systems
** Disk structure
   
** Disk attachment
** Disk scheduling
   + minimize seek time
   + seek time ≈ seek distance
*** FCFS
*** SSTF
*** SCAN(elevator algorithm)
** Disk management
** Swap-space management
** RAID
   + Disk *striping* uses a group of disks as one storage unit
* Chap 13 I/O systems
** I/O hardware
   + common concepts
     * *port* : hardware interface
     * *bus(daisy chain)*
     * *controller(host adapter)*: adpating between bus and device
   + [[./images/OperatingSystem/PCBus.png]]
   + device have addresses
     * special I/O instructions
     * *memory-mapped* I/O
*** I/O port registers
    + *Data-in*
    + *Data-out*
    + *Status*
    + *Control*
*** polling
    + determines state of device
      + command-ready
      + busy
      + error
*** interrupt
    + CPU *Interrupt-request* line triggered by I/O device
    + *Interrupt handler* receives interrupts
    + *Maskable* to ignore or delay some interrupts
    + various interrupt processing
      + page fault: saves the state of the process, moves it to the waiting queue,
        schedules another process to resume execution, then returns
      + trap:  saves the state of user code, switches to supervisor mode. Low priority
      + low priority interrupt
*** Direct memory access(DMA)
    + used to avoid *programmed I/O* for large data movement
    + requires *DMA* controller
    + Bypasses CPU to transfer data directly between I/O device and memory 
    + [[./images/OperatingSystem/DMA.png]]
      when DMA controller seizes mem bus, CPU is prevented from accessing memory
** Application I/O interface
   + I/O system calls encapsulate device behaviors in generic classes
   + device-driver layer hides differences among I/O controllers from kernel
*** block and character devices
    + block devices include disk drives
      - command include read, write, seek
*** network devices
*** Blocking and nonblocking I/O
    + *blocking*
    + *nonblocking* : I/O call returns as much as available
      + returns quickly with count of bytes read or writtern(unpredictable)
    + *asynchronous* 
      select
** Kernel I/O subsystem
   + *scheduling*
     - some I/O request ordering via per-device queue
   + *buffering*
   + *caching* - fast memory holding copy of data of the device
   + *spooling* - hold output for a device
     print
   + *device reservation*
** transforming I/O requests to hardware operations
   + reading a file from disk
     1. Determine device holding file 
        MS-DOS uses the c: disk id; Unix uses the mount table
     2. Translate name to device representation
     3. Physically read data from disk into buffer
     4. Make data available to requesting process
     5. Return control to process
   + [[./images/OperatingSystem/ReadFile.png]]
** streams
** performance
* Linux kernel: chap1
** OS kernel
*** component
    + interrupt handlers
    + scheduler
    + memory management system
    + system services
*** else
    + Kernel-space refers to the *elevated system state* (full access to hardware
      and its *protected memory space*
    + Applications execute a system call in kernel space, and the kernel is running in *process context*
    + The interrupt handlers run in an *interrupt context*, which is not associated with any process
** Contexts in linux
   + In user-space, executing user code in a process
   + In kernel-space, in process context, executing on behalf of a specific process
   + In kernel-space, in interrupt context, not associated with a process, handling an interrupt 
** Linux Layer
*** structure
    + [[./images/OperatingSystem/LayerStructure.png]]
** Features
   + *Dynamic loading* of modules
   + *SMP* support
   + A task can be preempted, when executing in the kernel
   + All threads are implemented as processes that *share resources*
   + Removed poorly designed Unix features (such as STREAMS)
** Subsystem
* Linux: chap2
** Kernel images
* Linux: chap4 process scheduling
** Multitasking developments
   + *cooperative multitasking* vs *preemptive multitasking*
   + constant-time scheduler O(1)
   + completely fair scheduler(CFS)
** scheduling policy
   + policy attempts to balance *fast process reponse time* and *maximal system utilization*
   + linux aims at good interactive response, favors I/O-bound processes
   + priority-based scheduling: higher priority run before lower priority
     + nice value [-20, 19], default 0 lower with higher priority
     + real-time priority
** time slice
   + time a process can run
   + I/O bound processes don't need longer TS
   + CPU-bound processes crave long TS(to keep their caches hot)
   + 
            Normal priority         Lowest priority
     Nice value   0                        +20 
     Time slice 100ms                      5ms
   + low process tends to go background
   + hard to decide absolute TS to allot each nice value
   + relative nice values are poorly mapped
** CFS
   + calculate how long each process should run as a function of the total # of
     runnable processes
   + CFS uses the nice value to weight the proportion of processor
   + CFS sets a *target latency* for the actual TS
     + is tl is 20ms, 2 tasks, each 10ms
     + 20 tasks, each 1ms
   + floor on target latency as *minimum granularity* for TS
   + Process selection
     + the *virtual runtime* is the actual runtime weighted by the # of runnable processes
     + when selecting a process to run next, it finds the process with the
       *smallest vruntime* from a red-black tree of processes
** 
* Linux: chap5 system calls
** syscall layer
   + provides an abstracted hardware interface for userspace
   + ensures system security and stability
   + allows for the virtualized system provided to processes
   + [[./images/OperatingSystem/Syscall.png]]
** syscall handler
   + user space application signals the kernel via a *software interrupt(exception)*
     and the system will switch to *kernel mode* and excute the *exception handler(syscall handler)*
     _int $0x80_
   + function *system_call()* implemented in entry_64.S
   + the syscall number is fed into the kernel via the *eax* register
   + return value sent via *eax*
   + parameters passed ria *register* ebx, ecx, edx, esi, edi
     more arguments are passed via *pointer*
** syscall implementation
   + defining its purpose, arguments, return value, error codes
** system call context
   + kernel is in *process context* during the excution of a syscall
   + kernel is capable of *sleeping* and is *preemptible* in process context
   + interrupt cannot sleep
     + sleepable means syscalls can make use of the majority of the kernel functionality
     + preemptible means the system calls must be reentrant(可以反复用，每个调用的人有自己的context)
** steps in binding a system call
   + add an *entry* to the end of the system call table
   + for each supported arch, define the *syscall number* in <asm/unistd.h>
* linux: chap6 kernel data structure
** Kernel data structure
   + linked lists
   + queues
   + maps
   + binary trees
* Linux: chap7 Interrupts
** Interrupts
   + polling
   + interrupt handler/interrupt service routine. The interrupt handler for a device is
     *part of device's driver*
   + kernel invokes interrupt handlers in an *interrupt context* - the kernel code that menages
     the devices
** Top halves and bottom halves
   + goals of interrupt handler
     1. execute quickly
     2. do a large amount of work
   + the interrupt handler is the *top half.* The top half is running immediately upon receipt of
     the interrupt
   + work can be later performed until the *bottom half*
   + a driver can register an interrupt handler
     _request_irq()_
     _free_irq()_
** Interrupt handler
   + declared by
     *static irqreturn_t intr_handler(int irq, void *dev)*
   + return value is *IRQ_NONE*, *IRQ_HANDLED*
   + if all the handlers on a given interrupt line return *IRQ_NONE*, the kernel
     will detect the problem
   + interrupt handlers in Linux need not be reentrant. When one handler is
     executing, the same line is masked out on all processors
     中断线不能复用
** Interrupt context
   + *interrupt context*
   + syscall - *process context* (macro *current* points to the associated task)
   + in interrupt context, the current macro is not relevant, thus it *cannot sleep* 
     (there is nothing to hang)
** Interrupt control
   + the kernel provides interfaces for disablign all interrupt
   + masking out an interrupt line
* Linux: chap8 bottom halves and deferring work
** Things interrupt handlers do not handle
   + interrupt handlers can form only the *first half* of any interrupt processing
     due to their limitation:
     1. They must be quick
     2. they cannot block
   + thus they are executed by the kernel asynchronously in immediate response to a
     h/w interrupt
** Bottom halves
   + The top halves - *perform as little work as possible*
   + the bottom halves - *take care of the rest*
   + device driver author split them
** Why bottom halves
   + need to defer work until any point in the future
   + bottom haves are run when
     1. system is less busy
     2. interrupts are again enabled
** real world bottom halves
   + softirqs :: 
   + tasklets
   + work queues
* Linux: kernel memory management
** Physical memory management
   + kernel treats *phisical pages* as the basic unit of management
   + an instance of the *page* structure is allocated for each frame
     #+BEGIN_SRC c
       struct page {
           unsigned long flags;
           atomic_t _count;
           atomic_t _mapcount;
           unsigned long private;
           struct address_space *mapping;
           pgoff_t index;
           struct list_head lru;
           void *virtual;
       };
     #+END_SRC
     physical address在代码中是看不到的
     virtual address
** Zone
   + Due to h/w limitation, pages are not treated equally
     + ZONE-DMA
     + ZONE-NORMAL
     + ZONE-HIGHMEM(pages not permanently mapped into the kernel space) only in 32-bit
** Allocating kenrel memory
   + allocating pages: *alloc_page()*, *alloc_pages()*
   + kmalloc vmalloc
   + why need contiguous memory
     on many architectures, hardware devices live on the other size of the memory management
     unit and don't understand virtual address
     vmalloc() needs to set up page table entries and have greater TLB thrashing
** Buddy system allocator
   + the minimum allocation unit is a *frame*
   + capable of allocating a contiguous number of frames
   + two contiguous groups of frames are buddies if they
     * are equal sized
     * physically contiguous
     * size is xⁿ
   + minimum unit is 1 page/frame
   + each element of array *free_area[]* manages the free blocks at k-th order
** Slab allocator
   + *free lists* are good for frequent allocation/deallocation of data structure
   + free lists in kernel has no global control, when available memory is low
   + the *slab layer* (allocator) acts as a generic data structure caching layer
     * one cache(*kmem_cache*) per object type, e.g. one cache for task_structure, another for
       inode objects
     * *kmalloc* interface is built on top of slab layer
     * caches are divided into slabs
     * slabs are composed of one or more physically contiguous frames
     * each slab contains a number of objects
* Linux: Virtual memory areas
** virtual memory areas(VMA)
   + each VMA has properties and a set of operations
   + *vm_area_struct*
   + {vm_start, vm_end} is the contiguous range for the VMA
   + if two separate processes map the same file to their respective address space,
     each has a *unique* vm_area_struct to identify its unique memory area
   + two threads sharing an address space also share all the VMA structures
** 
** 
** 
** 
* Linux
** Process address spaces
   + Each process is given a flat 32 or 64 bit address space
   + processes can choose to share their address space with others: there are threads
   + process can address up to 4GB, but it doesn't have permission to access all of it
** Kernel space stroed in physical memory
   + kernel space cannot be swapped out
   + the kernel space is mapped to the physical memory starting from 0x00000000
   + the kernel image is stored at 0x00100000
** The memory descriptor
   + A process's address space is described in a *mm_struct* defined in <linux/mm_types.h>
   + memory management information
   + *mm* field of *task_struct*
   + *copy_mm()* function copies the parent's *mm_struct* to its child during *fork*
   + *CLONE_VM*
** Kernel threads
   + kernel threads don't have a process address space. *mm* is NULL
   + This is the *definition* of a kernel thread - processes that have no user context
   + kernel threads don't access any *user-space memory*
* Linux: the linux filesystem
** Linux FCB
   + Linux filesystems(ext2, ext3 and ext4) separate storage for filename & FCB info.
     An FCB is stored as an inode. Each inode has a unique ID
   + Directory contains filename and inode info. Directory entry contains the filename
     and the inode ID
** File types
   + normal files
   + directories are files
   + Device files
   + FIFO files
   + symbolic links
   + sockets
** File system types
** /proc file system
* Linux: virtual file system
** vfs
   + vfs is not a disk filesystem
   + interoperate between various file systems
   + *filesystem abstraction layer*
     + abstracts file operations an APIs
** objects and data structures
   + superblock objet - super_operations
   + inode object - inode_operations
   + dentry object - dentry_operations
   + file object - file_operations
** superblock object
   + corresponds to the filesystem superblock or FCB
** Inode
   + 
* PThread library
  + int pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex)
    pthread_cond_wait() is used to block on a condition variable
    It's called with _mutex_ locked by the calling thread or undefined behaviour will result.
    the function atomically releases mutex and caused the calling thread to block on the condition
    variable
  + int pthread_cond_signal(pthread_cond_t *cond);
    int pthread_cond_broadcast(pthread_cond_t *cond);
#+NAME: condition variable
#+BEGIN_SRC c
  pthread_mutex_t count_lock;
  pthread_cond_t count_nonzero;

  void decrement_count() {
    pthread_mutex_lock(&count_lock);
    while(count == 0)
      //why count_locked is needed
      //count_locked is unlocked and locked
      pthread_cond_wait(&count_nonzero, &count_locked);
    count--;
    pthread_mutex_unlock(&count_lock);
  }

  void increment_count() {
    pthread_mutex_lock(&count_lock);
    if (count == 0)
      pthread_cond_signal(&count_nonzero);
    count++;
    pthread_mutex_unlock(&count_lock);
  }
#+END_SRC
  + Can we call cond pthread_cond_signal without locking mutex?
  + the *pthread_cond_signal()* routine is used to signal another thread which is waiting
    on the condition variable. It should be called after mutex is locked, and must
    unlock mutex in order for *pthread_cond_wait()*
#+NAME: example
#+BEGIN_SRC c
  //A
  pthread_mutex_lock(&mutex)
  while(condition == FALSE)
    pthread_cond_wait(&cond, &wait);
  pthread_mutex_unlock(&mutex);


  //B
  condition = TRUE;
  pthread_cond_signal(&cond)



  //////////A
  pthread_mutex_lock(&mutex)
  while(condition == FALSE)
    ///////B
    condition = TRUE;
  pthread_cond_signal(&cond)
  /////////A
  pthread_mutex_unlock(&mutex);


  //The signal will miss, then be blocked forever
#+END_SRC
* Homework
  + 
    | 100K | 500K | 200K | 300K | 600K |
    |      | 212K |      |      | 417K |
    |      | 112K |      |      |      |

    | 100K | 500K | 200K | 300K | 600K |
    |      | 417K | 112K | 212K | 426K |

    | 100K | 500K | 200K | 300K | 600K |
    |      | 417K |      |      | 212K |
    |      |      |      |      | 112K |
  + 
     | 1 | 2 | 3 | 4 | 6
     |   |   | 1 | 1 |
     1, 2, 3, 4, 5, 3, 4, 1, 6, 7, 8, 7, 8, 9, 7, 8, 9, 5, 4, 5, 4, 2.
                 5        6                             16    
     | 7 | 5 | 8 | 4 | 10 |
     |   |   |   |   |   |
  + 
    (125-86)+(1470-86)+(1470-913)+(1774-913)+(1774-948)+(1509-948)+(1509-1022)+(1750-1022)+(1750-130)
    (913-143)+(948-913)+(1022-948)+(1470-1022)+(1509-1470)+(1750-1509)+(1774-1750)+(1774-130)+(130-86)
    (913-143)+(948-913)+(1022-948)+(1470-1022)+(1509-1470)+(1750-1509)+(1774-1750)+(4999-1774)+4999+86+(130-86)
* Review
** chap1
   + OS definition, kernel
   + interrupt 
** 
** 
** 
** 
** 
** 
** 
** 
** 
** 
** 
** 
** 
