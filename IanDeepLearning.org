#+LATEX_HEADER: \usepackage{commath}
#+LATEX_HEADER: \newcommand{\bl}[1] {\boldsymbol{#1}}
%#+header: :imagemagick
* Chap2
   + $\boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}=\displaystyle\sum_ix_i\boldsymbol{A}_{:,i,}$ thus matrix can be regarded as a collection of vector
   + orthogonal matrix ::
     + rows are mutually orthonormal and columns are mutually orthonormal
       $\boldsymbol{A}^T\boldsymbol{A}=\boldsymbol{A}\boldsymbol{A}^T=\boldsymbol{I}$
   + eigendecomposition ::decompose matrix into a set of eigenvectors and eigenvalues
     + $\boldsymbol{Av}=\lambda\boldsymbol{v}$
     + If $\boldsymbol{A}$ has n linearly independent eigenvectors, then
       $\boldsymbol{V=}[\boldsymbol{v}^{(1)}, \dots, \boldsymbol{v}^{(n)}]$
       $\boldsymbol{\lambda}=[\lambda_1, \dots, \lambda_n]^T$
       $\boldsymbol{A=V}\text{diag}(\boldsymbol{\lambda})\boldsymbol{V}^{-1}$
   + Every real symmetric matrix can be decomposed into an expression using only
     real-valued eigenvectors and eigenvalues
     $\boldsymbol{A=Q\Lambda Q}^T$
     where $\boldsymbol{Q}$ is an orthogonal matrix composed of eigenvectors of $\boldsymbol{A}$
     $\boldsymbol{\Lambda}$ is a diagonal matrix associated with $\boldsymbol{Q}$
     $f(\boldsymbol{x})=\boldsymbol{x^TAx}$ subject to $||\boldsymbol{x}||_2=1$. Whenever $\boldsymbol{x}$ is equal to an
     eigenvector of $\boldsymbol{A}$, $f$ takes on the value of the corresponding
     eigenvalues.
   + singular :: Matrix is singular if and only if the eigenvalues are 0
   + positive definite :: all eigenvalues are positive
   + positive semidefinite ::
   + positive semidefinite matrices are interesting because they guarantee that
     $\forall\boldsymbol{x}, \boldsymbol{x^TAx}\ge 0$
   + Singular value decomposition ::
     + Every real matrix has a singular value decomposition
     + $\boldsymbol{A=UDV}^T$
       $\boldsymbol{A}$ is $m\times n$, $\boldsymbol{U}$ is $m\times m$, $\boldsymbol{D}$ is $m\times n$, $\boldsymbol{V}$ is $n\times n$
       $\boldsymbol{U,V}$ are orthogonal matrices, $\boldsymbol{D}$ is diagonal matrix and is not necessarily square
     + The elements along the $\boldsymbol{D}$ are *singular values*. The columns of $\boldsymbol{U}$ are
       *left-singular vectors*. The columns of $\boldsymbol{V}$ are *right-singular vectors*
* Chap4
** Poor condition
    $f(\boldsymbol{x})=\boldsymbol{A^{-1}x}$. When $\boldsymbol{A}\in \mathbb{R}^{n\times n}$ has an eigenvalue decomposition, its *condition number* is
    $\displaystyle\text{max}_{i,j}|\frac{\lambda_i}{\lambda_j}|$. When this number is large, matrix inversion is particularly sensitive to
    error in the input
** Gradient-based optimization
    + objective function or criterion :: the function we want to minimize
         when we are minimizing it, we may also call it *cost function, loss function*
         or *error function*
    + We denote the value that minimized or maxmizes with a superscipt *
*** Jocobian and Hessian matrices
     + Jocobian matrix :: the matrix containing all partial derivatives of a function
          whose input and output are vectors
       + $\boldsymbol{f}:\mathbb{R}^m\to \mathbb{R}^n$, then Jocobian matrix $\boldsymbol{J}$ of $\boldsymbol{f}$ is $\boldsymbol{J}_{i,j}=\frac{\partial}{\partial x_j}f(\boldsymbol{x})_i$
     + Hessian matrix :: collection of second derivatives
       + $\boldsymbol{H}(f)(\boldsymbol{x})_{i,j}=\frac{\partial^2}{\partial x_i\partial x_j}f(\boldsymbol{x})$
       + Hessian is the Jocobian of the gradient
       + Anywhere that the second partial derivatives are continuous, the differential
         operators are commutative.$\frac{\partial^2}{\partial x_i\partial x_j}f(\boldsymbol{x})=\frac{\partial^2}{\partial x_j\partial x_i}f(\boldsymbol{x})$
         Thus Hessian matrix is symmetric
         Since it's real and symmetric, we can *decompose* it into a set of real eigenvalues
         and an orthogonal basis of eigenvectors
       + The second derivative in a specific direction represented by a unit vector $\boldsymbol{d}$
         is given by $\boldsymbol{d^THd}$. When $\boldsymbol{d}$ is an eigenvector of $\boldsymbol{H}$, the second derivative in that
         direction is given by the corresponding eigenvalue. For other direction, the
         directinal second derivative is a weighted average of all of the eigenvalues,
         with weights between 0 and 1
       + The second derivative tells us how well we can expect a gradient descent stop to perform
         We can make a second-order Tayler series approximation to the function $f(\boldsymbol{x})$
         around the current point $\boldsymbol{x}^{(0)}$:
         \begin{equation*}
         f(\boldsymbol{x})\approx f(\boldsymbol{x}^{(0)})+(\boldsymbol{x-x^{(0)}})^T\boldsymbol{g}+
         \frac{1}{2}(\boldsymbol{x-x^{(0)}})^T\boldsymbol{H(x-x)^{(0)}}
         \end{equation*}
         where $\boldsymbol{g, H}$ are at $\boldsymbol{x}^{(0)}$
         If using a learning rate $\epsilon$, new point $\boldsymbol{x=x}^{(0)}-\epsilon\boldsymbol{g}$
         \begin{equation*}
         f(\boldsymbol{x^{(0)}-\epsilon g})\approx f(\boldsymbol{x}^{(0)})-\epsilon\boldsymbol{g^Tg}
         +\frac{1}{2}\epsilon^2\boldsymbol{g^THg}
         \end{equation*}
         When $\boldsymbol{g^THg}$ is positive. then $\epsilon^*=\frac{\boldsymbol{g^Tg}}{\boldsymbol{g^THg}}$, the worst case is $\frac{1}{\lambda_{max}}$
       + If $f''(x)>0$ and $'f(x)=0$, it's a local minimum
         In multiple dimensions, we need to examine all of the second derivatives of the function
         At a critical point where $\bigtriangledown_xf(\boldsymbol{x})=0$, when *Hessian* is positive definite, the point
         is local minimum.
         This can be seen that the directional second derivative in any direction must be
         positive
       + Newton's method ::
                           $\boldsymbol{x^*=x^{(0)}-H}(f)(\boldsymbol{x}^{(0)})^-1\triangledown_{\boldsymbol{x}} f(\boldsymbol{x}^{(0)})$
       + convex optimization algorithms are applicable only to convex functions--functions for
         which the Hessian is positive semidefinite everywhere
** Constrained optimization
   + Some times we wish to find the maximal or minimal value of $f(\boldsymbol{x})$ for values
     of $\boldsymbol{x}$ in some set $\mathbb{S}$
   + Karush-Kuhn-Tucker(KKT) ::
     + generalized Langrangian or generalized Lagerange function ::
       + $\mathbb{S}$ ::
         + we want a description of $\mathbb{S}$ in terms of $m$ functions $g^{(i)}$ and
           n functions $h^{(i)}$ so that $\mathbb{S}=\{\boldsymbol{x}\mid\forall i,g^{(i)}(\boldsymbol{x})=0
           \quad\text{and}\quad \forall j,h^{(j)}(\boldsymbol{x})\le 0\}$
           *equality constraints* and *inequality constraints*
     + $L(\boldsymbol{x,\lambda,\alpha})=f(\boldsymbol{x})+\displaystyle\sum_i\lambda_ig^{i}(\boldsymbol{x})+\displaystyle\sum_j\alpha_jh^{(j)}(\boldsymbol{x})$
       $\lambda_i,\alpha_j$ are called *KKT multipliers*
     + So long as at least one feasible point exists and $f(\boldsymbol{x})$ is not permitted to have value $\infty$
       then $\min_{\boldsymbol{x}}\max_{\boldsymbol{\lambda}}\max_{\boldsymbol{\alpha,\alpha}\ge 0}
       L(\boldsymbol{x,\lambda,\alpha})$ has the same optimal objective function value and set of
       optimal points $\boldsymbol{x}$ as $\min_{\boldsymbol{x}\in\mathbb{S}}f(\boldsymbol{x})$
       Because any time the constraints are satisfied,
       $\displaystyle\max_{\boldsymbol{\lambda}}\max_{\boldsymbol{\alpha,\alpha}\ge 0}L(
       \boldsymbol{x,\lambda,\alpha})=f(\boldsymbol{x})$
       while any time a constraint is violated$\displaystyle\max_{\boldsymbol{\lambda}}\max_{\boldsymbol{\alpha,\alpha}\ge 0}L(
       \boldsymbol{x,\lambda,\alpha})=\infty$
     + Inequality constraints ::
          + We say a constraint $h^{(i)}(\boldsymbol{x})$ is *active* if $h^{(i)}(\boldsymbol{x}^*)=0$
** Example
   Find $\boldsymbol{x}$ minimizes $f(\boldsymbol{x})=\frac{1}{2}||\boldsymbol{Ax-b}||_2^2$
* Chap5
** Learning algorithms
*** Task
    + Classification ::
    + Classification with missing inputs ::
      Computer program is not guaranteed that every measurement in its input
      vector will always be provided.
      + One way to define:
        Learn a probability distribution over all of the relevant variables,
        then solve the classification task by marginalizing out the missing
        variables
    + Regression ::
      + Program is asked to predicted a numerical value given some input
      + $\mathbb{R}^n\to\mathbb{R}$
    + Transcription ::
      + the system is asked to observe a relatively unstructured representation
        of some kind of data and transcribe it into discrete
      + Speech recognition
    + Translation ::
    + Structured output ::
      + output is a vector (or some other data structure) with important relationships
        between the different elements.
      + Parsing, Pixel segmentation
    + Anomaly detection ::
      + credit card fraud detection
    + Synthesis and sampling ::
      + Generate new examples that are similar to those in the training data
    + Imputation of missing values ::
      + Given a new example $\boldsymbol{x}\in\mathbb{R}$, but with some entries $x_i$ of $\boldsymbol{x}$ missing,
        predict missing entries
    + Denoising ::
    + Density estimation / probability mass function estimation ::
      + The algorithm is asked to learn a function $p_{\text{model}}:\mathbb{R}^n\to\mathbb{R}$, where $p_{\text{model}}(\boldsymbol{x})$ can
        be insterpreted as probability density function
*** Performance measure
    + We often refer to the error rate as the expected 0-1 loss
    + For tasks such as density estimation, we report the average log-probability
      the model assigns to some examples
    + We use *test set* to measure
*** The experience
    + Unsupervised learning algorithms ::
      + experience a dataset containing many features, then learn useful properites
        of the structure of this dataset
      + Given vector $\boldsymbol{x}$, learn the distribution $p(\boldsymbol{x})$ or other properties
        e.g. we could use chain rules $p(\boldsymbol{x})=\displaystyle\prod_{i=1}^np(x_i\mid x_1,\dots,x_{i-1})$ to predict
    + Supervised learning algorithms ::
      + Each example is also associated with a *label* or *target*.
      + predict $\boldsymbol{y}$ from $\boldsymbol{x}$
        $p(y\mid\boldsymbol{x})=\frac{p(\boldsymbol{x}, y)}{\sum_{y'}p(\boldsymbol{x},y')}$
*** Linear regression
    + Goal :: $\mathbb{R}^n\to\mathbb{R}$
    + Output :: $\hat{y}=\boldsymbol{w^Tx}$
    + One way of measuring the performance is the *mean squared error*
      $0&=\text{MSE}_{\text{test}}=\frac{1}{m}\displaystyle\sum_i(\hat{\boldsymbol{y}}^{(\text{test})}-
      \boldsymbol{y}^{(\text{test})})_i^2$ or $\text{MSE}_{\text{test}}=\frac{1}{m}\norm{
      \hat{\boldsymbol{y}}^{(\text{test})}-\boldsymbol{y}^{(\text{test})}}_2^2$
      \begin{align*}
      0&=\nabla_{\bl{w}}(\bl{X}^{\text{train}}\bl{w-y}^{\text{train}})^T
      (\bl{X}^{train}\bl{w-y}^{train})\\&=\nabla_{\bl{w}}(\bl{w^TX}^{train T}\bl{X}^{train}
      \bl{w-2w^TX}^{train T}\bl{y}^{train}+\bl{y}^{train T}\bl{y}^{train})\\&=
      2\bl{X}^{trainT}\bl{X}^{train}\bl{w-2X}^{trainT}\bl{y}^{train}=0
      \end{align*}
      $\bl{w}=(\bl{X}^{trainT}\bl{X}^{train})^{-1}\bl{X}^{trainT}\bl{y}^{train}$ this is the *normal equation*
    + Linear regression ::
         $\hat{y}=\bl{w^Tx}+b$
** Capacity, Overfitting and Underfitting
*** front
   + generalization ::
                      the ability to perform well on previously unobserved inputs
   + statistical learning theory view ::
     + data generating process ::
       + train and test data are generated by a probability distribution over datasets
     + i.i.d. assumption ::
       + independent ::
         + each dataset are independent from each other
       + identically distributed ::
       + shared underlying distribution is called *data generating distribution*
         denoted $p_{\text{data}}$
   + underfitting ::
     + model is not able to obtain a sufficiently low error value on the
         training set
   + overfitting ::
     + gap between the training error and test error is too large
   + capacity ::
     + ability to fit a wide variety of function
     + models with low capacity may struggle to fit the training set
     + models with high capacity can overfit by memorizing properties of the
       training set
   + Hypothesis space ::
     + the set of functions that the learning algorithm is allowed to select as
       being the solution
     + e.g. linear regression has the set of all linear functions of its input as
       hypothesis space
   + representational capacity ::
     + the model specifies which family of functions the learning algorithm can
       choose from.
   + Occam's razor ::
   + Vapnik-Chervonenkis dimension ::
     + VC dimension measures the capacity of a binary classifier.
     + Defined as being the largest possible value of $m$ for which there exists a
       training set of $m$ different $\bl{x}$ points that the classifier can label
       arbitrarily
   + Non-parametric models ::
     + nearest neighbor regression ::
       + Simply stores the $\bl{X}$ and $\bl{y}$ from the training set. When asked to classify a
         test point $\bl{x}$, the model looks up the nearest entry in the training set
         and returns the associated regression target.
         $\hat{y}=y_i$ where $i=\text{arg}\min\norm{\bl{X}_{i,;}-\bl{x}}_2^2$
   + Bayes error ::
                   the error incurred by an oracle making predictions from the true
                   distribution $p(\bl{x},y)$
*** The no free lunch theorem
    + Unlike logic requiring every menber of a set, ML avoids this problem by offering
      probabilistic rules
    + No free lunch theorem ::
      + averaged over all possible data generating distributions, every classfication
        has the same error rate when classifying previously unobserved points.
      + No ML algorithm is universally any better than any other
    + Regularization ::
      + We can use *weight decay* :
        $J(\bl{w})=\text{MSE}_{\text{traine}}+\lambda\bl{w}^T\bl{w}$
        Minimizing $J(\bl{w})$ results in a choice of weights between fitting the training
        data and being small
      + Regularization is any modification we make to a learning algorithm that is
        intended to reduce its generalization error but not its training error
** Hyperparameters and validation sets
   + We split the training data into two disjoint subsets. One is used to learn parameter
     and the other is *validation* set to estimate the generalization error during or
     after training, allowing for the hyperparameters to be updated accordingly.
   + Cross-validation ::
     + A small test set implies statistical uncertainty around the estimated average
       test error
** Estimator, Bias and variance
*** Point estimation
    + Point estimation is the attempt to provide the single "best" prediction of some
      quantity of interest
    + Let $\{\bl{x}^{(1)},\dots,\bl{x}^{(m)}\}$ be a set of $m$ independent and identically distributed data points.
      A *point estimator* or *statistic* is any function of the data:
      $\hat{\bl{\theta}}_m=g(\bl{x}^{(1)},\dots,\bl{x}^{(m)})$
    + Function estimation ::
      + try to predict $\bl{y}$ given in input $\bl{x}$
      + Assume $f(\bl{x})$ describes the approximation relationship between $\bl{y,x}$
      + If $\bl{y}=f(\bl{x})+\bl{\epsilon}$ where $\bl{\epsilon}$ stands for part of $\bl{y}$ but not predictable from $\bl{x}$
*** Bias
    + $\text{bias}(\hat{\bl{\theta}}_m)=\mathbb{E}(\hat{\bl{\theta}}_m)-\bl{\theta}$
      *unbiased* if bias=0
      *asymptotically unbiased* if $\lim_{m\to\infty}\text{bias}(\hat{\bl{\theta}}_m)=0$
*** Variance and standard error
*** Trading off Bias and Variance to minimize MSE
    + $\text{MSE}=\mathbb{E}\[(\hat{\theta}_m-\theta)\]=\text{Bias}(\hat{\theta}_m)^2+\text{Var}(\hat{\theta}_m)$
*** Consistency
    + We would like $\text{plim}_{m\to\infty}\hat{\theta}_m=\theta$
      where plim is convergence in probability, meaning $\forall\epsilon>0,P(|\hat{\theta}_m-
      \theta|>\epsilon)\to 0$ as $m\to\infty$
    + Almost sure convergence ::
         For a sequence of random variables $\bl{x}^{(1)},\dots$ to a value $\bl{x}$ occurs
         when $p(\lim_{m\to\infty}\bl{x}^{(m)}=\bl{x})=1$
** Maximum likelihood estimation
** Bayesian statistics
   consider all possible values of $\bl{\theta}$ when making a prediction
   + Bayesian inference ::
     + Prior distribution ::
          what you know about parameter $\beta$, excluding the information in
          the data - denoted $\pi(\beta)$
     + Likelihood ::
                    based on modeling assumptions, how (relatively) likely the data $Y$
                    are if the truth is $\beta$ - denoted $f(Y\mid\beta)$
     +
       \begin{align*}
       p(\bl{\beta\mid Y})\quad\propto\quad f(\bl{Y\mid\beta})\times\pi(\bl{\beta})\\
       \text{Posterior}\quad\propto\quad \text{Likelihood}\times\text{Prior}
       \end{align*}
   + Before observing the data, we represent our knowledge of $\bl{\theta}$ using
     *prior probability distribution*, $p(\bl{\theta})$
   + A data samples $\{x^{(1)},\dots,x^{(m)}\}$, we can recover the effect of data on our belief
     about $\bl{\theta}$ by combining the data likelihood $p(x^{(1)},\dots,x^{(m)}\mid\bl{\theta})$ with the prior
     $p(\bl{\theta}\mid x^{(1)},\dots,x^{(m)})=\frac{p(x^{(1)},\dots,x^{(m)}\mid\bl{\theta})
     p(\bl{\theta})}{p^{(1)},\dots,p^{(m)}}$
   + Unlike maximum likelihood approach, *Bayesian* make predictions using a full
     distribution over $\bl{\theta}$. For example, after observing $m$ examples,
     $p(x^{(m+1)}\mid x^{(1)},\dots,x^{(m)})=\displaystyle\int p(x^{(m+1)}\mid\bl{\theta})p(\bl{\theta}\mid
     x^{(1)},\dots,x^{(m)})d\bl{\theta}$
*** Basic knowledge
    + Covariance matrix $\Sigma$ ::
      + a matrix whose element in the i,j position is the covariance between
        $i^{th}$ and $j^{th}$ elements of a random vector
    + Multivariate normal distribution ::
      + A random vector $\bl{X}=(X_1,\dots,X_k)$ has multivariate normal distribution if it
        satisfies the following equivalent conditions
        * Every linear combination of its components $Y=a_1X_1+\dots+a_kX_k$ is normally
          distributed. $Y=\bl{a^TX}$
        * There exists a random $l$-vector $\bl{Z}$, whose components are independent
          standard normal random variables, a $k$-vector $\bl{\mu}$ and a $k\times l$
          matrix $\bl{A}$ s.t. $\bl{X=AZ+\mu}$
      + When $\Sigma$ is *positive definite*
        $f_{\bl{X}}(x_1,\dots,x_k)=\frac{\exp(-\frac{1}{2}\bl{(x-\mu)^T\sum^{-1}(x-\mu)})}
        {\sqrt{(2\pi)^k|\bl{\sum}|}}$
        $|\Sigma|\equiv \bl{\text{det}\Sigma}$
*** Example: Bayesian Linear regression
    Consider the Bayesian estimation approach to learn the linear regression
    parameters
    + $\hat{y}=\bl{w^Tx}$
    + Given a set of $m$ training samples $(\bl{X}^{(\text{train})},\bl{y}^{(\text{train})})$
      $\hat{\bl{y}}^{(\text{train})}=\bl{X}^{(\text{train})}\bl{w}$
    + Guassian distribution ::
         $f(x\mid\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$
    + Expressed as a Gaussian conditional distribution on $\bl{y}^{(\text{train})}$
      $p(\bl{y}^{(\text{train})}\mid\bl{X}^{(\text{train})},\bl{w})=\mathcal{N}(\bl{y}^{(train)};
      \bl{X}^{(\text{train})}\bl{w,I})\propto\exp{(-\frac{1}{2}(\bl{y}^{(\text{train})}-\bl{X}^{(\text{train})}\bl{w})^T(
      \bl{y}^{(\text{train})}-\bl{X}^{(\text{train})}\bl{w}))}$
    + For real-valued parameters it's common to use a Gaussian as a prior distribution
      $p(\bl{w})=\mathcal{N}(\bl{w;\mu_0,\Lambda_o})\propto(-\frac{1}{2}(\bl{w-\mu_0})^T
      \Lambda_0^{-1}(\bl{w-\mu_0}))$
      where $\bl{\mu_0}$ and $\bl{\Lambda}_0$ are the prior distribution mean vector and covariance matrix
      $\bl{\Lambda}_0=\text{diag}(\bl{\lambda})_0$
    + w
      \begin{align*}
      p(\bl{w\mid X,y})&\propto p(\bl{y\mid X, w})p(\bl{w})\\
      &\propto\exp(-\frac{1}{2}(\bl{y-Xw}^T(\bl{y-Xw})))\exp(-\frac{1}{2}(\bl{w-\mu_0})^T
      \bl{\Lambda}_0^{-1}(\bl{w-\mu_0}))\\
      &\propto(-\frac{1}{2}(-2\bl{y^TXw+w^TX^TXw+w^T\Lambda_0^{-1}w-2\mu^T_0\Lambda_0^{-1}w}))
      \end{align*}
      then define $\bl{\Lambda}_m=(\bl{X^TX+\Lambda_0^{-1}})^{-1}$, and $\bl{\mu_m}=\bl{\Lambda_m(X^Ty+\Lambda_0^{-1}\mu_o)}$
      \begin{align*}
      p(\bl{w\mid X,y})&\propto \exp(-\frac{1}{2}\bl{(w-\mu_m)^T\Lambda_m^{-1}(w-\mu_m)
      +\frac{1}{2}\mu_m^T\Lambda_m^{-1}\mu_m})\\
      &\propto(-\frac{1}{2}\bl{(w-\mu_m)^T\Lambda_m^{-1}(w-\mu_m)}
      \end{align*}
** Supervised learning algorithms
*** Probabilistic supervised learning
