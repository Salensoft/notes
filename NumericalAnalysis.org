#+TITLE: Numerical Analysis
#+AUTHOR: gouziwu
#+LATEX_HEADER: \usepackage{xcolor, amsthm, mathabx, mathtools, pgfplots,amsmath}
#+LATEX_HEADER: \newtheorem{theorem}{Theorem}[section]
#+LATEX_HEADER: \newtheorem{definition}{Definition}[section]
#+LATEX_HEADER: \newtheorem{corollary}{Corollary}[section]
#+LATEX_HEADER: \newtheorem{lemma}{Lemma}[section]
#+LATEX_HEADER: \newcommand{\bl}[1] {\boldsymbol{#1}}



* Chap1 Mathematical Preliminaries
** 1.2 Roundoff Errors and Computer Arithmetic
   *Truncation Error* : the error involved in using a truncated, or finite, summation to
   approximate the sum of an infinite series 

   *Roundoff Error*: the error produced when performing real number calculations.
   It occurs because the arithmetic performed in a machine involves numbers
   with only a finite number of digits. 


   Suppose $y=\textcolor{blue}{0.d_1d_2\dots
   d_k}d_{k+1}d_{k+2}\dots\textcolor{blue}{\times 10^n{}}$, then
 
   $fl(y)=\begin{cases} 0.d_1d_2\dots d_k\times 10^n&\quad\text{chopping}\\
   chop(y+5\times 10^{n-(k+1)})=0.\delta_1\delta_2\dots \delta_k\times
   10^n&\quad\text{Rounding}\\\end{cases}$

    
   \begin{definition}
   If $p*$ is an approximation to $p$, the \textcolor{red}{absolute error} is $|p-p*|$,
   and the \textcolor{red}{relative error} is $\frac{|p-p*|}{|p|}$, provided that $p\neq 0$
   \end{definition}

   \begin{definition}
   The number $p*$ is said to approximate $p$ to $t$
   \textcolor{red}{significant digits} if $t$ is the largest nonnegative
   integer for which $\frac{|p-p*|}{|p|}<5\times 10^{-t}$
   \end{definition}

   + chopping ::
                 $|\frac{y-fl(y)}{y}|=|\frac{0.d_1d_2\dots d_kd_{k+1}\dots
                 \times 10^n-0.d_1d_2\dots d_k\times 10^n}{0.d_1d_2\dots
                 d_kd_{k+1}\times
                 10^n}|=|\frac{0.d_{k+1}\dots}{0.d_1d_2\dots}|\times 10^{-k}\le
                 \frac{1}{0.1}\times 10^{-k}=10^{-k+1}$
   + rounding ::
                 $|\frac{y-fl(y)}{y}|\le \frac{0.5}{0.1}\times 10^{-k}=0.5\times
                 10^{-k+1}$

   *Finite digit arithmetic*
   
   + $x\oplus y=fl(fl(x)+fl(y))$
   + $x\otimes y=fl(fl(x)\times fl(y))$
   + $x\ominus y=fl(fl(x)-fl(y))$
   + $x\odiv y=fl(fl(x)\div fl(y))$
   
** 1.3 ALgorithms and Convergence
   An algorithm that satisfies that small changes in the initial data produce
   correspondingly small changes in the final results is called *stable*;
   otherwise it is *unstable*. An algorithm is called *conditionally stable* if it
   is stable only for certain choices of initial data. 

   Suppose that E₀ > 0 denotes an initial error and En represents the magnitude
   of an error after n subsequent operations. If $E_n\approx CnE_0$, where C is a
   constant independent of n, then the growth of error is said to be *linear*. If
   $E_n\approx C^nE_0$, for some C > 1, then the growth of error is called *exponential* 
   
   Suppose $\{\beta_n\}_{n=1}^\infty, \lim\limits_{n \to \infty}\beta_n=0,
   \{\alpha_n\}_{n=1}^\infty, \lim\limits_{n\to\infty}\alpha_n=\alpha$.
   If a positive constant K exists with $|\alpha_n-\alpha|\le K|\beta_n|$ for
   large n, then $\{\alpha_n\}_{n=1}^\infty$ converges to α with *rate, or*
   *order, of convergence* $O(\beta_n)$

   Suppose $\lim\limits_{h\to 0}G(h)=0, \lim\limits_{h\to 0}F(h)=L$ and
   $|F(h)-L|\le K|G(h)|$ for sufficiently small h, then we write
   $F(h)=L+O(G(h))$
* Chap2 Solutions of equations in one variable
** 2.1 Bisection method
   \begin{theorem}{Intermediate Value Theorem}
   If $f\in C[a,b]$, $K\in(f(a), f(b))$, then there exists a number $p\in(a,b)$
   for which $f(p)=K$
   \end{theorem}

   \begin{theorem}
   Suppose that $f\in C[a,b]$ and $f(a)\cdot f(b)<0$. The bisection method
   generates a sequence $\{p_n\},n=0,1,\dots$ approximating a zero $p$ of $f$ with
   \begin{equation*}
   |p_n-p|\le\frac{b-a}{2^n}, \quad\text{when } n\ge 1
   \end{equation*}
   \end{theorem}
** 2.2 Fixed-Point Iteration
   $f(x)=0\xleftrightarrow{\text{equivalent}} x=f(x)+x=g(x)$

   \begin{theorem}{Fixed-Point Theorem}
   Let $g\in C[a,b]$ be s.t. $g(x)\in[a,b]$ for all $x\in[a,b]$. Suppose that
   $g'$ exists on $(a,b)$ and that a constant $0<k<1$ exists with $|g'(x)|\le k$
   for all $x\in(a,b)$ (hence $g'$ can't converge to 1). Then for any number
   $p_0$ in $[a,b]$, the sequence defined by $p_n=g(p_{n-1}), n\ge 1$ converges
   to the unique point $p$ in $[a,b]$
   \end{theorem}

   \begin{corollary}
   $|p_n-p|\le\frac{1}{1-k}|p_{n+1}-p_n|$ and
   $|p_n-p|\le\frac{k^n}{1-k}|p_1-p_0|$
   \end{corollary}
** 2.3 Newton's method
   Linearize a nonlinear function using *Taylor's expansion*

   Let $p_0\in [a,b]$ be an approximation to $p$ s.t. $f'(p_0)\neq 0$, hence 
   $f(x)=f(p_0)+f'(p_0)(x-p_0)+\frac{f''(\xi_x)}{2!}(x-p_0)^2$, then
   $0=f(p)\approx f(p_0)+f'(p_0)(p-p0)\rightarrow p\approx
   p_0-\frac{f(p_0)}{f'(p_0)}$
   $p_n=p_{n-1}-\frac{f(p_{n-1})}{f'(p_{n-1})},\quad\text{for} n\ge 1$

   \begin{theorem}
   Let $f\in C^2[a,b]$. If $p\in[a,b]$ is s.t. $f(p)=0,f'(p)\neq0$, then there
   exists a $\delta>0$ s.t. Newton's method generates a sequence $\{p_n\},
   n\in\mathbb{N}\setminus\{0\}$ converging to $p$ for any initial approximation
   $p\in[p-\delta,p+\delta]$.
   \end{theorem}
** 2.4 Error analysis for iterative methods
   \begin{definition}
   Suppose $\{p_n\}(n=0,1,\dots)$ is a sequence that converges to $p$ with
   $p_n\neq p$ for all $n$. If positive constants $\alpha$ and $\lambda$ exist
   with
   \begin{equation*}
   \lim\limits_{n\to\infty}\frac{|p_{n+1}-p|}{|p_n-p|^\alpha}=\lambda
   \end{equation*}
   then $\{p_n\}(n=0,1,\dots)$ \textcolor{red}{converges to p of order
   $\alpha$, with asymptotic error constant $\lambda$}
   \end{definition}

   \begin{theorem}
   Let $p$ be a fixed point of $g(x)$. If there exists some constant $\alpha\ge
   2$ s.t. $g\in C^\alpha[p-\delta,p+\delta]$,
   \textcolor{red}{$g'(p)=\dots=g^{\alpha-1}(p)=0$} and \textcolor{red}{$g^\alpha(p)\neq 0$}.
   Then the iterations with $p_n=g(p_{n-1})$, $n\ge1$ is of \textcolor{red}{order $\alpha$}
   \end{theorem}

   \begin{equation*}
   p_{n+1}=g(p_n)=g(p)+g'(p)(p_n-p)+\dots+\frac{g^\alpha(\xi_n)}{\alpha!}(p_n-p)^\alpha
   \end{equation*}

   \begin{theorem}
   Let $g\in C[a,b]$ be s.t. $g(x)\in[a,b]$ for all $x\in[a,b]$. Suppose in
   addition that $g'$ is continuous on $(a,b)$ and a positive constant $k<1$
   exists with
   \begin{equation*}
   |g'(x)|\le k, \quad \text{for all } x\in(a,b)
   \end{equation*}
   If $g'(p)\neq0$, then for any number $p_0\neq p$ in $[a,b]$, the sequence
   \begin{equation*}
   p_n=g(p_{n-1}),\quad\text{for }n\ge 1
   \end{equation*}
   converges only linearly to the unique fixed point in $[a,b]$
   \end{theorem}
   
   \begin{proof}
   \begin{align*}
   \lim\limits_{n\to\infty}\frac{|p_{n+1}-p|}{|p_n-p|}&=
   \lim\limits_{n\to\infty}\frac{|g(p_n)-p|}{|p_n-p|}\\
   &=\lim\limits_{n\to\infty}\frac{|g'(\xi)(p_n-p)|}{|p_n-p|}\\
   &=|g'(p)|
   \end{align*}
   \end{proof}

   \begin{theorem}
   Let $p$ be a solution of the equation $x=g(x)$. Suppose that $g'(p)=0$ and
   g'' is continuous with $|g''(x)|<M$ on an open interval $I$ containing $p$.
   Then there exists a $\delta>0$ s.t. for $p_0\in[p-\delta,p+\delta]$, the
   sequence defined by $p_n=g(p_{n-1})$, when $n\ge 1$ converges at least
   quadratically to $p$. Moreover, for sufficiently large values of $n$,
   \begin{equation*}
   |p_{n+1}-p|<\frac{M}{2}|p_n-p|^2
   \end{equation*}
   \end{theorem}
   
   \begin{proof}
   Choose $k\in(0,1),\delta>0$ s.t. $[p-\delta,p+\delta]\subseteq I$ and
   $|g'(x)|<k$ and $g''$ is continuous.
   \begin{equation*}
   g(x)=g(p)+g'(p)(x-p)+\frac{g''(\xi)}{2}(x-p)^2
   \end{equation*}
   Hence $g(x)=p+\frac{g''(\xi)}{2}(x-p)^2$.
   $p_{n+1}=g(p_n)=p+\frac{g''(\xi_n)}{2}(p_n-p)^2$. Thus
   $p_{n+1}-p=\frac{g''(\xi_n)}{2}(p_n-p)^2$. We get
   \begin{equation*}
   \lim\limits_{n\to\infty}\frac{|p_{n+1}-p|}{|p_n-p|^2}=\frac{g''(p)}{2}
   \end{equation*}
   \end{proof}

   \begin{definition}
   A solution $p$ of $f(x) = 0$ is a \textcolor{red}{zero of multiplicity} $m$
   of $f$ if for $x\neq p$, $f(x)=(x-p)^mq(x)$ where $\lim\limits_{x\to
   p}q(x)\neq 0$
   \end{definition}

   \begin{theorem}
   The function $f\in C^m[a,b]$ has a zero of multiplicity $m$ at $p$ in $(a,b)$
   if and only if
   \begin{equation*}
   0=f(p)=f'(p)=\dots=f^{(m-1)}(p),\quad\text{but } f^{(m)}(p)\neq 0
   \end{equation*}
   \end{theorem}

   To handle the problem of multiple roots of a function $f$ is to define
   $\mu(x)=\frac{f(x)}{f'(x)}$.

   If p is a zero of f of multiplicity m with $f(x)=(x-p)^mq(x
)$, then
   \begin{align*}
   \mu(x)&=\frac{(x-p)^mq(x)}{m(x-p)^{m-1}q(x)+(x-p)^mq'(x)}\\
   &=(x-p)\frac{q(x)}{mq(x)+(x-p)q'(x)}
   \end{align*}
   And $q(x)\neq 0$.

   Now Newton's method:
   \begin{align*}
   g(x)&=x-\frac{\mu(x)}{\mu'(x)}\\
   &=x-\frac{f(x)/f'(x)}{(f'(x)^2-f(x)f''(x))/f'(x)^2}\\
   &=x-\frac{f(x)f'(x)}{f'(x)^2-f(x)f''(x)}
   \end{align*}
* Chap6 Direct Methods for Solving Linear Systems
** 6.1 Linear Systems of Equations
   *Gaussian elimination with backward substitution*
** 6.2 Pivoting Strategies
   *Problem*: small pivot element may cause trouble
   
   *Paritial Pivoting*: Determine the smallest p≥k s.t.
    $|a_{pk}^{(k)}|=\displaystyle\max_{k\le j\le n}|a_{ik}^{(k)}|$ and
    interchange the pth and the kth rows
    
    *Scaled Partial Pivoting*:
    1. Define a scale factor $s_i$ for each row as $s_i=\displaystyle\max_{1\le
       j\le n}|a_{ij}|$
    2. Determine the smallest $p\ge k$ s.t.
       $\frac{|a_{pk}^{(k)}}{s_p}=\displaystyle\max_{k\le i\le
       n}\frac{|a_{ik}^{(k)}|}{s_i}$
       and interchange the pth and the kth rows

       
    *Complete Pivoting*: Search all the entries $a_{ij}$ to find the entry with
     the largest magnitude
** 6.5 Matrix Factorization
   $m_{ik}=a_{ik}/a_{kk}$
   \begin{equation*}
   L_k=
   \begin{pmatrix}
   1 &            &            &               &  \\
     & \ddots     &            &\mbox{\Huge 0} &  \\
     &            & 1          &               &  \\
     &            & -m_{k+1,k} &               &  \\
     &            & \vdots     & \ddots        &  \\
     &            & -m_{n,k}   &               & 1\\
   \end{pmatrix}
   \end{equation*}  


   Hence 
   
   \begin{equation*}
   L_1^{-1}L_2^{-1}\dots L_{n-1}^{-1}=
   \begin{pmatrix}
   1&&&\mbox{\Huge 0}\\
   &1&&\\
   &&\ddots&\\
   \text{\Huge $m_{i,j}$}&&&1\\
   \end{pmatrix}
   \end{equation*}

   \begin{equation*}
   U=
   \begin{pmatrix}
   a_{11}&a_{12}&\dots&a_{1n}\\
   &a_{22}&\dots&a_{2n}\\
   &&\dots&\vdots\\
   &&&a_{nn}\\
   \end{pmatrix}
   \end{equation*}

   $A=LU$
** 6.6 Special Types of Matrices
   *Strictly Diagonally Dominant Matrix*.
   $|a_{ii}|>\displaystyle\sum_{\substack{j=1,\\j\neq i}}^n|a_{ij}| \quad
   \text{for each } i=1,\dots,n$

   \begin{theorem}
   A strictly diagonally dominant matrix A is \textcolor{red}{nonsingular}. Moreover,
   Gaussian elimination can be performed \textcolor{red}{without} row or column
   \textcolor{red}{interchanges}, and the computations will be \textcolor{red}{stable}
   w.r.t. the growth of roundoff errors
   \end{theorem}

   *Choleski's Method for Positive Definite Matrix*:
   \begin{definition}
   A matrix A is \textcolor{red}{positive definite} if ti's symmetric and if    
   $ \mathbf{x}^T \mathbf{A} \mathbf{x}>0$ for every n-dimensional vector $ \mathbf{x}\neq 0$
   \end{definition}

   \begin{lemma}
   A is positive definite
   \begin{enumerate}
   \item $A^{-1}$ is positive definite as well, and $a_{ii}>0$
   \item $\sum|a_{ij}|\le\max|a_{kk}|$; $(a_{ij})^2<a_{ii}a_{jj}$ for each i ≠ j
   \item Each of /A's leading principal submatrices $A_k$/ has a positive determinant
   \end{enumerate}
   \end{lemma}

   \begin{equation*}
   U =
   \begin{pmatrix}
   &u_{ij}\\
   &&\\
   \end{pmatrix}=
   \begin{pmatrix}
   u_{11}&&\\
   &\ddots&\\
   &&u_{nn}\\
   \end{pmatrix}
   \begin{pmatrix}
   1&&u_{ij}/u_{ii}\\
   &1&\\
   &&1\\
   \end{pmatrix}=D\tilde{U}
   \end{equation*}
   A is symmetric, hence 
   \begin{equation*}
   L=\tilde{U}^t, A=LDL^t
   \end{equation*}
   Let 
   \begin{equation*}
   D^{1/2}=
   \begin{pmatrix}
   \sqrt{u_{11}}&&\\
   &\ddots&\\
   &&\sqrt{u_{nn}}\\
   \end{pmatrix}, \tilde{L}=LD^{1/2/}, A=\tilde{L}\tilde{L}^t
   \end{equation*}
   
   *Crout Reduction for tridiagonal Linear System*

   \begin{equation*}
   \begin{pmatrix}
   b_1 & c_1    &        &        &\\
   a_2 & b_2    & c_2    &        &\\
       & \ddots & \ddots & \ddots &\\
       &        & a_{n-1}& b_{n-1}& c_{n-1} \\
       &        &        & a_n    & b_n\\
   \end{pmatrix}
   \begin{pmatrix}
   x_1\\
   x_2\\
   \vdots\\
   x_{n-1}\\
   x_n
   \end{pmatrix}=
   \begin{pmatrix}
   f_1\\
   f_2\\
   \vdots\\
   f_{n-1}\\
   f_n
   \end{pmatrix}
   \end{equation*}

   \begin{equation*}
   A=
   \begin{pmatrix}
   \alpha_1 &&&\\
   \gamma_2 & \ddots &&\\
            & \ddots & \ddots   &\\
            &        & \gamma_n & \alpha_n\\
   \end{pmatrix}
   \begin{pmatrix}
   1 & \beta_1 &&\\
     & \ddots & \ddots &\\
     &        & \ddots & \beta_{n-1}\\
     &        &        & 1\\
   \end{pmatrix}
   \end{equation*}
* Chap7 Iterative techiniques in Matrix algebra
** 7.1 Norms of vectors and matrices
   \begin{definition}
   A \textcolor{red}{vector norm} on $R^n$ is a function $||\cdot||: \mathbb{R}^n\to \mathbb{R}$
   with following properties for all $ \mathbf{x,y}\in \mathbb{R}^n, \alpha\in C$
   \begin{enumerate}
   \item $|| \mathbf{x}||\le 0$; $|| \mathbf{x}||=0\Longleftrightarrow \mathbf{x}= \mathbf{0}$
   \item $||\alpha \mathbf{x}||=|\alpha|\cdot|| \mathbf{x}||$
   \item $|| \mathbf{x}+ \mathbf{y}||\le|| \mathbf{x}||+|| \mathbf{y}||$
   \end{enumerate}
   \end{definition}

   $|| \mathbf{x}||_1=\displaystyle\sum_{i=1}^n|x_i|$.
   $||\mathbf{x}_p||=(\displaystyle\sum_{i=1}^n|x_i|^p)^{1/p}$
   
   \begin{definition}
   A sequence $\{\mathbf{x}^{(k)}\}_{k=1}^\infty$ of vectors in $R^n$ 
   \textcolor{red}{converge to} $\mathbf{x}$ w.r.t the norm $||\cdot||$ if
   given any $\epsilon>0$ there exists an integer $N(\epsilon)$ s.t.
   $||\mathbf{x}^{(k)}-\mathbf{x}||<\epsilon$ for all $k\ge N(\epsilon)$
   \end{definition}

   \begin{theorem}
   The sequence of vectors $\{\mathbf{x}^{(k)}\}$ converges to $ \mathbf{x}\in R^n$
   w.r.t. $||\cdot||$ if and only if $ \lim\limits_{k\to\infty}\mathbf{x}^{(k)}_i=x_i$
   for each $i=1,2,\dots,n$
   \end{theorem}

   \begin{definition}
   If there exist positive constants $C_1,C_2$ s.t. $C_1||\mathbf{x}||_B\le||\mathbf{x}||_A
   \le C_2||\mathbf{x}|_B|$. Then $||\cdot||_A,||\cdot||_B$ are \textcolor{red}{equivalent} 
   \end{definition}

   \begin{theorem}
   All the vector norm in $R^n$ are equivalent
   \end{theorem}


   \begin{definition}
   A \textcolor{red}{matrix norm} on the set of $n\times n$:
   \begin{enumerate}
   \item $||\mathbf{A}||\ge0;||\mathbf{A}||=0\Longleftrightarrow \mathbf{A}=\mathbf{0}$
   \item $||\alpha \mathbf{A}||=|\alpha|\cdot||\mathbf{A}||$
   \item $||\mathbf{A}+\mathbf{B}||\le||\mathbf{A}||+||\mathbf{B}||$
   \item $||\mathbf{AB}||\le||\mathbf{A}||\cdot||\mathbf{B}||$
   \end{enumerate}
   \end{definition}
  
   *Frobenius Norm*: $||\mathbf{A}||_F=\sqrt{\displaystyle\sum_{i=1}^n
   \displaystyle\sum_{j=1}^n|a_{ij}|^2}$

   *Natural Norm*: $||\mathbf{A}||_p=\displaystyle\max_{\mathbf{x}\neq
   \mathbf{0}}\frac{||\mathbf{Ax}||_p}{||\mathbf{x}||_p}=\displaystyle\max_{\mathbf{z}\neq
   \mathbf{0}}||\mathbf{A}\frac{\mathbf{z}}{||\mathbf{z}||}||=\displaystyle\max_{||\mathbf{x}||_p=1}||\mathbf{Ax}||_p$

   $||\mathbf{A}||_\infty=\displaystyle\max_{1\le i\le n}\displaystyle\sum_{j=1}^n|a_{ij}|$,
   $||\mathbf{A}||_1=\displaystyle\max_{1\le j\le n}\displaystyle\sum_{i=1}^n|a_{ij}|$,
   $||\mathbf{A}||_2=\sqrt{\lambda_\text{max}(\mathbf{A}^T \mathbf{A})}$
** 7.4 Error bounds and iterative refinement
   Assume that A is accurate and $\bl{b}$ has the error $\delta \bl{b}$,
   then $\bl{A}(\bl{x}+\delta \bl{x})=\bl{b}+\delta \bl{b}$

   \begin{theorem}
   Suppose $\tilde{\bl{x}}$ is an approximation to the solution of $ \bl{Ax=b}$
   A is nonsingular matrix. Then for any natural norm,
   \begin{equation*}
   ||\bl{x-\tilde{x}}||\le||\bl{r}||\cdot||A^{-1}||
   \end{equation*}
   and if $ \bl{x\neq 0, b\neq 0}$,
   \begin{equation*}
   \frac{||\delta\bl{x}||}{||\bl{x}||}\le||\bl{A}
   ||\cdot||\bl{A}^{-1}||\cdot \frac{||\delta\bl{b}||}{||\bl{b}||}
   \end{equation*}
   \end{theorem}

   \begin{proof}
   $\bl{r=b-A\tilde{x}}=A\bl{x}-A\tilde{\bl{x}}$ and A is nonsingular. Hence 
   $\bl{x-\tilde{x}}=A^{-1}\bl{r}$. Since $\frac{||A^{-1}\bl{r}||}{||\bl{r}||}\le||A^{-1}||$
   , $||\bl{x-\tilde{x}}||=||A^{-1}\bl{x}||\le||A^-1||\cdot||\bl{r}||$. Also
   $||\bl{b}||\le||A||\cdot||\bl{x}||$. So $1/||\bl{x}||\le||A||/||\bl{b}||$
   \end{proof}

   \begin{theorem}
   If a matrix B satisfies $||B||<1$ for some natural norm, then
   \begin{enumerate}
   \item $I\plusmn$
   \end{enumerate}
   \end{theorem}
