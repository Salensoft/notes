% Created 2019-04-14 日 17:27
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{xcolor, amsthm, mathabx, mathtools, pgfplots,amsmath}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newcommand{\bl}[1] {\boldsymbol{#1}}
\author{gouziwu}
\date{\today}
\title{Numerical Analysis}
\hypersetup{
 pdfauthor={gouziwu},
 pdftitle={Numerical Analysis},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.1 (Org mode 9.1.14)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents




\section{Chap1 Mathematical Preliminaries}
\label{sec:orgd2328e7}
\subsection{1.2 Roundoff Errors and Computer Arithmetic}
\label{sec:orgdfcf641}
\textbf{Truncation Error} : the error involved in using a truncated, or finite, summation to
approximate the sum of an infinite series 

\textbf{Roundoff Error}: the error produced when performing real number calculations.
It occurs because the arithmetic performed in a machine involves numbers
with only a finite number of digits. 


Suppose \(y=\textcolor{blue}{0.d_1d_2\dots
   d_k}d_{k+1}d_{k+2}\dots\textcolor{blue}{\times 10^n{}}\), then

\(fl(y)=\begin{cases} 0.d_1d_2\dots d_k\times 10^n&\quad\text{chopping}\\
   chop(y+5\times 10^{n-(k+1)})=0.\delta_1\delta_2\dots \delta_k\times
   10^n&\quad\text{Rounding}\\\end{cases}\)


\begin{definition}
If $p*$ is an approximation to $p$, the \textcolor{red}{absolute error} is $|p-p*|$,
and the \textcolor{red}{relative error} is $\frac{|p-p*|}{|p|}$, provided that $p\neq 0$
\end{definition}

\begin{definition}
The number $p*$ is said to approximate $p$ to $t$
\textcolor{red}{significant digits} if $t$ is the largest nonnegative
integer for which $\frac{|p-p*|}{|p|}<5\times 10^{-t}$
\end{definition}

\begin{description}
\item[{chopping}] \(|\frac{y-fl(y)}{y}|=|\frac{0.d_1d_2\dots d_kd_{k+1}\dots
                 \times 10^n-0.d_1d_2\dots d_k\times 10^n}{0.d_1d_2\dots
                 d_kd_{k+1}\times
                 10^n}|=|\frac{0.d_{k+1}\dots}{0.d_1d_2\dots}|\times 10^{-k}\le
                 \frac{1}{0.1}\times 10^{-k}=10^{-k+1}\)
\item[{rounding}] \(|\frac{y-fl(y)}{y}|\le \frac{0.5}{0.1}\times 10^{-k}=0.5\times
                 10^{-k+1}\)
\end{description}

\textbf{Finite digit arithmetic}

\begin{itemize}
\item \(x\oplus y=fl(fl(x)+fl(y))\)
\item \(x\otimes y=fl(fl(x)\times fl(y))\)
\item \(x\ominus y=fl(fl(x)-fl(y))\)
\item \(x\odiv y=fl(fl(x)\div fl(y))\)
\end{itemize}

\subsection{1.3 ALgorithms and Convergence}
\label{sec:org81e4f8f}
An algorithm that satisfies that small changes in the initial data produce
correspondingly small changes in the final results is called \textbf{stable};
otherwise it is \textbf{unstable}. An algorithm is called \textbf{conditionally stable} if it
is stable only for certain choices of initial data. 

Suppose that E₀ > 0 denotes an initial error and En represents the magnitude
of an error after n subsequent operations. If \(E_n\approx CnE_0\), where C is a
constant independent of n, then the growth of error is said to be \textbf{linear}. If
\(E_n\approx C^nE_0\), for some C > 1, then the growth of error is called \textbf{exponential} 

Suppose \(\{\beta_n\}_{n=1}^\infty, \lim\limits_{n \to \infty}\beta_n=0,
   \{\alpha_n\}_{n=1}^\infty, \lim\limits_{n\to\infty}\alpha_n=\alpha\).
If a positive constant K exists with \(|\alpha_n-\alpha|\le K|\beta_n|\) for
large n, then \(\{\alpha_n\}_{n=1}^\infty\) converges to α with \textbf{rate, or}
\textbf{order, of convergence} \(O(\beta_n)\)

Suppose \(\lim\limits_{h\to 0}G(h)=0, \lim\limits_{h\to 0}F(h)=L\) and
\(|F(h)-L|\le K|G(h)|\) for sufficiently small h, then we write
\(F(h)=L+O(G(h))\)
\section{Chap2 Solutions of equations in one variable}
\label{sec:org8298845}
\subsection{2.1 Bisection method}
\label{sec:orgd7704dc}
\begin{theorem}{Intermediate Value Theorem}
If $f\in C[a,b]$, $K\in(f(a), f(b))$, then there exists a number $p\in(a,b)$
for which $f(p)=K$
\end{theorem}

\begin{theorem}
Suppose that $f\in C[a,b]$ and $f(a)\cdot f(b)<0$. The bisection method
generates a sequence $\{p_n\},n=0,1,\dots$ approximating a zero $p$ of $f$ with
\begin{equation*}
|p_n-p|\le\frac{b-a}{2^n}, \quad\text{when } n\ge 1
\end{equation*}
\end{theorem}
\subsection{2.2 Fixed-Point Iteration}
\label{sec:org2021e79}
\(f(x)=0\xleftrightarrow{\text{equivalent}} x=f(x)+x=g(x)\)

\begin{theorem}{Fixed-Point Theorem}
Let $g\in C[a,b]$ be s.t. $g(x)\in[a,b]$ for all $x\in[a,b]$. Suppose that
$g'$ exists on $(a,b)$ and that a constant $0<k<1$ exists with $|g'(x)|\le k$
for all $x\in(a,b)$ (hence $g'$ can't converge to 1). Then for any number
$p_0$ in $[a,b]$, the sequence defined by $p_n=g(p_{n-1}), n\ge 1$ converges
to the unique point $p$ in $[a,b]$
\end{theorem}

\begin{corollary}
$|p_n-p|\le\frac{1}{1-k}|p_{n+1}-p_n|$ and
$|p_n-p|\le\frac{k^n}{1-k}|p_1-p_0|$
\end{corollary}
\subsection{2.3 Newton's method}
\label{sec:org38cde5d}
Linearize a nonlinear function using \textbf{Taylor's expansion}

Let \(p_0\in [a,b]\) be an approximation to \(p\) s.t. \(f'(p_0)\neq 0\), hence 
\(f(x)=f(p_0)+f'(p_0)(x-p_0)+\frac{f''(\xi_x)}{2!}(x-p_0)^2\), then
\(0=f(p)\approx f(p_0)+f'(p_0)(p-p0)\rightarrow p\approx
   p_0-\frac{f(p_0)}{f'(p_0)}\)
\(p_n=p_{n-1}-\frac{f(p_{n-1})}{f'(p_{n-1})},\quad\text{for} n\ge 1\)

\begin{theorem}
Let $f\in C^2[a,b]$. If $p\in[a,b]$ is s.t. $f(p)=0,f'(p)\neq0$, then there
exists a $\delta>0$ s.t. Newton's method generates a sequence $\{p_n\},
n\in\mathbb{N}\setminus\{0\}$ converging to $p$ for any initial approximation
$p\in[p-\delta,p+\delta]$.
\end{theorem}
\subsection{2.4 Error analysis for iterative methods}
\label{sec:org92acf7f}
\begin{definition}
Suppose $\{p_n\}(n=0,1,\dots)$ is a sequence that converges to $p$ with
$p_n\neq p$ for all $n$. If positive constants $\alpha$ and $\lambda$ exist
with
\begin{equation*}
\lim\limits_{n\to\infty}\frac{|p_{n+1}-p|}{|p_n-p|^\alpha}=\lambda
\end{equation*}
then $\{p_n\}(n=0,1,\dots)$ \textcolor{red}{converges to p of order
$\alpha$, with asymptotic error constant $\lambda$}
\end{definition}

\begin{theorem}
Let $p$ be a fixed point of $g(x)$. If there exists some constant $\alpha\ge
2$ s.t. $g\in C^\alpha[p-\delta,p+\delta]$,
\textcolor{red}{$g'(p)=\dots=g^{\alpha-1}(p)=0$} and \textcolor{red}{$g^\alpha(p)\neq 0$}.
Then the iterations with $p_n=g(p_{n-1})$, $n\ge1$ is of \textcolor{red}{order $\alpha$}
\end{theorem}

\begin{equation*}
p_{n+1}=g(p_n)=g(p)+g'(p)(p_n-p)+\dots+\frac{g^\alpha(\xi_n)}{\alpha!}(p_n-p)^\alpha
\end{equation*}

\begin{theorem}
Let $g\in C[a,b]$ be s.t. $g(x)\in[a,b]$ for all $x\in[a,b]$. Suppose in
addition that $g'$ is continuous on $(a,b)$ and a positive constant $k<1$
exists with
\begin{equation*}
|g'(x)|\le k, \quad \text{for all } x\in(a,b)
\end{equation*}
If $g'(p)\neq0$, then for any number $p_0\neq p$ in $[a,b]$, the sequence
\begin{equation*}
p_n=g(p_{n-1}),\quad\text{for }n\ge 1
\end{equation*}
converges only linearly to the unique fixed point in $[a,b]$
\end{theorem}

\begin{proof}
\begin{align*}
\lim\limits_{n\to\infty}\frac{|p_{n+1}-p|}{|p_n-p|}&=
\lim\limits_{n\to\infty}\frac{|g(p_n)-p|}{|p_n-p|}\\
&=\lim\limits_{n\to\infty}\frac{|g'(\xi)(p_n-p)|}{|p_n-p|}\\
&=|g'(p)|
\end{align*}
\end{proof}

\begin{theorem}
Let $p$ be a solution of the equation $x=g(x)$. Suppose that $g'(p)=0$ and
g'' is continuous with $|g''(x)|<M$ on an open interval $I$ containing $p$.
Then there exists a $\delta>0$ s.t. for $p_0\in[p-\delta,p+\delta]$, the
sequence defined by $p_n=g(p_{n-1})$, when $n\ge 1$ converges at least
quadratically to $p$. Moreover, for sufficiently large values of $n$,
\begin{equation*}
|p_{n+1}-p|<\frac{M}{2}|p_n-p|^2
\end{equation*}
\end{theorem}

\begin{proof}
Choose $k\in(0,1),\delta>0$ s.t. $[p-\delta,p+\delta]\subseteq I$ and
$|g'(x)|<k$ and $g''$ is continuous.
\begin{equation*}
g(x)=g(p)+g'(p)(x-p)+\frac{g''(\xi)}{2}(x-p)^2
\end{equation*}
Hence $g(x)=p+\frac{g''(\xi)}{2}(x-p)^2$.
$p_{n+1}=g(p_n)=p+\frac{g''(\xi_n)}{2}(p_n-p)^2$. Thus
$p_{n+1}-p=\frac{g''(\xi_n)}{2}(p_n-p)^2$. We get
\begin{equation*}
\lim\limits_{n\to\infty}\frac{|p_{n+1}-p|}{|p_n-p|^2}=\frac{g''(p)}{2}
\end{equation*}
\end{proof}

\begin{definition}
A solution $p$ of $f(x) = 0$ is a \textcolor{red}{zero of multiplicity} $m$
of $f$ if for $x\neq p$, $f(x)=(x-p)^mq(x)$ where $\lim\limits_{x\to
p}q(x)\neq 0$
\end{definition}

\begin{theorem}
The function $f\in C^m[a,b]$ has a zero of multiplicity $m$ at $p$ in $(a,b)$
if and only if
\begin{equation*}
0=f(p)=f'(p)=\dots=f^{(m-1)}(p),\quad\text{but } f^{(m)}(p)\neq 0
\end{equation*}
\end{theorem}

To handle the problem of multiple roots of a function \(f\) is to define
\(\mu(x)=\frac{f(x)}{f'(x)}\).

If p is a zero of f of multiplicity m with \(f(x)=(x-p)^mq(x
)\), then
\begin{align*}
\mu(x)&=\frac{(x-p)^mq(x)}{m(x-p)^{m-1}q(x)+(x-p)^mq'(x)}\\
&=(x-p)\frac{q(x)}{mq(x)+(x-p)q'(x)}
\end{align*}
And \(q(x)\neq 0\).

Now Newton's method:
\begin{align*}
g(x)&=x-\frac{\mu(x)}{\mu'(x)}\\
&=x-\frac{f(x)/f'(x)}{(f'(x)^2-f(x)f''(x))/f'(x)^2}\\
&=x-\frac{f(x)f'(x)}{f'(x)^2-f(x)f''(x)}
\end{align*}
\section{Chap3 Interpolation and polynomial approximation}
\label{sec:org9666754}
\subsection{3.1 Interpolation and the Lagrange polynomial}
\label{sec:org56371d3}
\(P_n(x)=\displaystyle\sum_{i=0}^nL_{n,i}(x)y_i\). Find \(L_{n,i}(x)\) for
\(i=0,\dots,n\) s.t. \(L_{n,j}(x_j)=\delta_{ij}\). \(\delta_{ij}\) Kronecker delta.
Each \(L_{n,i}\) has n roots \(x_0,\dots,\hat{x_i},\dots,x_n\).
\(L_{n,j}(x)=C_i(x-x_0)\dots\hat{(x-x_i)}\dots(x-x_n)=C_i \displaystyle
   \prod_{\substack{j\neq i\\j=0}}^n(x-x_j)\).
\(L_{n,j}(x_i)=1\to C_i=\displaystyle\prod_{j\neq i}\frac{1}{x_i-x_j}\).
Hence \(L_{n,i}(x)=\displaystyle\prod_{\substack{j\neq i\\j=0}}^n
   \frac{x-x_j}{x_i-x_j}\)

\begin{theorem}
If $x_0,x_1,\dots,x_n$ are n+1 distinct numbers and $f$ is a function whose values
are given at these numbers, then the n-th Lagrange interpolating polynomial 
is unique
\end{theorem}


\textbf{Analyze the remainder}. Suppose \(a\le x_0<x_1<\dots<x_n\le b\) and \(f\in
   C^{n+1}[a,b]\). Consider \(R_n(x)=f(x)-P_n(x)\).
\(R_n(x)\) has at least n+1 roots =>
\(R_n(x)=K(x)\displaystyle\prod_{i=1}^n(x-x_i)\).
For any \(x\neq x_i\). Define
\(g(t)=R_n(t)-K(x)\displaystyle\prod_{i=0}^n(t-x_i)\). \(g(x)\) has n+2 distinct
roots \(x_0\dots x_n x\). Hence \(g^{(n+1)}(\xi_x)=0,\xi_x\in(a,b)\).
\(f^{(n+1)}(\xi_x)-Pn^{(n+1)}(\xi_x)-K(x)(n+1)!=R_n^{(n+1)}(\xi_x)-K(x)(n+1)!\).
Thus \(R_n(x)=\frac{f^{(n+1)}(\xi_x)}{(n+1)!}\displaystyle\prod_{i=0}^n(x-x_i)\).
\section{Chap6 Direct Methods for Solving Linear Systems}
\label{sec:org3925d9a}
\subsection{6.1 Linear Systems of Equations}
\label{sec:org9aab32c}
\textbf{Gaussian elimination with backward substitution}
\subsection{6.2 Pivoting Strategies}
\label{sec:org9d3185f}
\textbf{Problem}: small pivot element may cause trouble

\textbf{Paritial Pivoting}: Determine the smallest p≥k s.t.
 \(|a_{pk}^{(k)}|=\displaystyle\max_{k\le j\le n}|a_{ik}^{(k)}|\) and
 interchange the pth and the kth rows

\textbf{Scaled Partial Pivoting}:
\begin{enumerate}
\item Define a scale factor \(s_i\) for each row as \(s_i=\displaystyle\max_{1\le
       j\le n}|a_{ij}|\)
\item Determine the smallest \(p\ge k\) s.t.
\(\frac{|a_{pk}^{(k)}}{s_p}=\displaystyle\max_{k\le i\le
       n}\frac{|a_{ik}^{(k)}|}{s_i}\)
and interchange the pth and the kth rows
\end{enumerate}


\textbf{Complete Pivoting}: Search all the entries \(a_{ij}\) to find the entry with
 the largest magnitude
\subsection{6.5 Matrix Factorization}
\label{sec:org2886d3f}
\(m_{ik}=a_{ik}/a_{kk}\)
\begin{equation*}
L_k=
\begin{pmatrix}
1 &            &            &               &  \\
  & \ddots     &            &\mbox{\Huge 0} &  \\
  &            & 1          &               &  \\
  &            & -m_{k+1,k} &               &  \\
  &            & \vdots     & \ddots        &  \\
  &            & -m_{n,k}   &               & 1\\
\end{pmatrix}
\end{equation*}  


Hence 

\begin{equation*}
L_1^{-1}L_2^{-1}\dots L_{n-1}^{-1}=
\begin{pmatrix}
1&&&\mbox{\Huge 0}\\
&1&&\\
&&\ddots&\\
\text{\Huge $m_{i,j}$}&&&1\\
\end{pmatrix}
\end{equation*}

\begin{equation*}
U=
\begin{pmatrix}
a_{11}&a_{12}&\dots&a_{1n}\\
&a_{22}&\dots&a_{2n}\\
&&\dots&\vdots\\
&&&a_{nn}\\
\end{pmatrix}
\end{equation*}

\(A=LU\)
\subsection{6.6 Special Types of Matrices}
\label{sec:org0fe84f8}
\textbf{Strictly Diagonally Dominant Matrix}.
\(|a_{ii}|>\displaystyle\sum_{\substack{j=1,\\j\neq i}}^n|a_{ij}| \quad
   \text{for each } i=1,\dots,n\)

\begin{theorem}
A strictly diagonally dominant matrix A is \textcolor{red}{nonsingular}. Moreover,
Gaussian elimination can be performed \textcolor{red}{without} row or column
\textcolor{red}{interchanges}, and the computations will be \textcolor{red}{stable}
w.r.t. the growth of roundoff errors
\end{theorem}

\textbf{Choleski's Method for Positive Definite Matrix}:
\begin{definition}
A matrix A is \textcolor{red}{positive definite} if ti's symmetric and if    
$ \mathbf{x}^T \mathbf{A} \mathbf{x}>0$ for every n-dimensional vector $ \mathbf{x}\neq 0$
\end{definition}

\begin{lemma}
A is positive definite
\begin{enumerate}
\item $A^{-1}$ is positive definite as well, and $a_{ii}>0$
\item $\sum|a_{ij}|\le\max|a_{kk}|$; $(a_{ij})^2<a_{ii}a_{jj}$ for each i ≠ j
\item Each of /A's leading principal submatrices $A_k$/ has a positive determinant
\end{enumerate}
\end{lemma}

\begin{equation*}
U =
\begin{pmatrix}
&u_{ij}\\
&&\\
\end{pmatrix}=
\begin{pmatrix}
u_{11}&&\\
&\ddots&\\
&&u_{nn}\\
\end{pmatrix}
\begin{pmatrix}
1&&u_{ij}/u_{ii}\\
&1&\\
&&1\\
\end{pmatrix}=D\tilde{U}
\end{equation*}
A is symmetric, hence 
\begin{equation*}
L=\tilde{U}^t, A=LDL^t
\end{equation*}
Let 
\begin{equation*}
D^{1/2}=
\begin{pmatrix}
\sqrt{u_{11}}&&\\
&\ddots&\\
&&\sqrt{u_{nn}}\\
\end{pmatrix}, \tilde{L}=LD^{1/2/}, A=\tilde{L}\tilde{L}^t
\end{equation*}

\textbf{Crout Reduction for tridiagonal Linear System}

\begin{equation*}
\begin{pmatrix}
b_1 & c_1    &        &        &\\
a_2 & b_2    & c_2    &        &\\
    & \ddots & \ddots & \ddots &\\
    &        & a_{n-1}& b_{n-1}& c_{n-1} \\
    &        &        & a_n    & b_n\\
\end{pmatrix}
\begin{pmatrix}
x_1\\
x_2\\
\vdots\\
x_{n-1}\\
x_n
\end{pmatrix}=
\begin{pmatrix}
f_1\\
f_2\\
\vdots\\
f_{n-1}\\
f_n
\end{pmatrix}
\end{equation*}

\begin{equation*}
A=
\begin{pmatrix}
\alpha_1 &&&\\
\gamma_2 & \ddots &&\\
         & \ddots & \ddots   &\\
         &        & \gamma_n & \alpha_n\\
\end{pmatrix}
\begin{pmatrix}
1 & \beta_1 &&\\
  & \ddots & \ddots &\\
  &        & \ddots & \beta_{n-1}\\
  &        &        & 1\\
\end{pmatrix}
\end{equation*}
\section{Chap7 Iterative techiniques in Matrix algebra}
\label{sec:org6193d1b}
\subsection{7.1 Norms of vectors and matrices}
\label{sec:orgc50e4b5}
\begin{definition}
A \textcolor{red}{vector norm} on $R^n$ is a function $||\cdot||: \mathbb{R}^n\to \mathbb{R}$
with following properties for all $ \mathbf{x,y}\in \mathbb{R}^n, \alpha\in C$
\begin{enumerate}
\item $|| \mathbf{x}||\le 0$; $|| \mathbf{x}||=0\Longleftrightarrow \mathbf{x}= \mathbf{0}$
\item $||\alpha \mathbf{x}||=|\alpha|\cdot|| \mathbf{x}||$
\item $|| \mathbf{x}+ \mathbf{y}||\le|| \mathbf{x}||+|| \mathbf{y}||$
\end{enumerate}
\end{definition}

\(|| \mathbf{x}||_1=\displaystyle\sum_{i=1}^n|x_i|\).
\(||\mathbf{x}_p||=(\displaystyle\sum_{i=1}^n|x_i|^p)^{1/p}\)

\begin{definition}
A sequence $\{\mathbf{x}^{(k)}\}_{k=1}^\infty$ of vectors in $R^n$ 
\textcolor{red}{converge to} $\mathbf{x}$ w.r.t the norm $||\cdot||$ if
given any $\epsilon>0$ there exists an integer $N(\epsilon)$ s.t.
$||\mathbf{x}^{(k)}-\mathbf{x}||<\epsilon$ for all $k\ge N(\epsilon)$
\end{definition}

\begin{theorem}
The sequence of vectors $\{\mathbf{x}^{(k)}\}$ converges to $ \mathbf{x}\in R^n$
w.r.t. $||\cdot||$ if and only if $ \lim\limits_{k\to\infty}\mathbf{x}^{(k)}_i=x_i$
for each $i=1,2,\dots,n$
\end{theorem}

\begin{definition}
If there exist positive constants $C_1,C_2$ s.t. $C_1||\mathbf{x}||_B\le||\mathbf{x}||_A
\le C_2||\mathbf{x}|_B|$. Then $||\cdot||_A,||\cdot||_B$ are \textcolor{red}{equivalent} 
\end{definition}

\begin{theorem}
All the vector norm in $R^n$ are equivalent
\end{theorem}


\begin{definition}
A \textcolor{red}{matrix norm} on the set of $n\times n$:
\begin{enumerate}
\item $||\mathbf{A}||\ge0;||\mathbf{A}||=0\Longleftrightarrow \mathbf{A}=\mathbf{0}$
\item $||\alpha \mathbf{A}||=|\alpha|\cdot||\mathbf{A}||$
\item $||\mathbf{A}+\mathbf{B}||\le||\mathbf{A}||+||\mathbf{B}||$
\item $||\mathbf{AB}||\le||\mathbf{A}||\cdot||\mathbf{B}||$
\end{enumerate}
\end{definition}

\textbf{Frobenius Norm}: \(||\mathbf{A}||_F=\sqrt{\displaystyle\sum_{i=1}^n
   \displaystyle\sum_{j=1}^n|a_{ij}|^2}\)

\textbf{Natural Norm}: \(||\mathbf{A}||_p=\displaystyle\max_{\mathbf{x}\neq
   \mathbf{0}}\frac{||\mathbf{Ax}||_p}{||\mathbf{x}||_p}=\displaystyle\max_{\mathbf{z}\neq
   \mathbf{0}}||\mathbf{A}\frac{\mathbf{z}}{||\mathbf{z}||}||=\displaystyle\max_{||\mathbf{x}||_p=1}||\mathbf{Ax}||_p\)

\(||\mathbf{A}||_\infty=\displaystyle\max_{1\le i\le n}\displaystyle\sum_{j=1}^n|a_{ij}|\),
\(||\mathbf{A}||_1=\displaystyle\max_{1\le j\le n}\displaystyle\sum_{i=1}^n|a_{ij}|\),
\(||\mathbf{A}||_2=\sqrt{\lambda_\text{max}(\mathbf{A}^T \mathbf{A})}\)
\subsection{7.2 Eigenvalues and Eigenvectors}
\label{sec:org827c3be}
\textbf{spectral radius}.
\begin{definition}
The \textcolor{red}{spectral radius $\rho(A)$} of a matrix A is defined as
$\rho(A)=\max|\lambda|$ where $\lambda$ is an eigenvalue of A
\end{definition}

\begin{theorem}
If A is an $n\times n$ matrix, then $\rho(A)\le||A||$ for any natural norm
\end{theorem}

\begin{proof}
$|\lambda|\cdot||\bl{x}||=||\lambda\bl{x}||=||A\bl{x}||\le||A||\cdot||\bl{x}||$
\end{proof}

\begin{definition}
We call an $n\times n$ matrix A \textcolor{red}{convergent} if for all $i,j=1,\dots,n$
$\lim\limits_{k\to\infty}(A^k)_{ij}=0$
\end{definition}
\subsection{7.3 Iterative techniques for solving linear systems}
\label{sec:orgc24bca8}
\textbf{Jacobi iterative method}.
\begin{equation*}
\begin{cases}
a_{11}x_1+a_{12}x_2+\dots+a_{1n}x_n=b_1\\
a_{21}x_1+a_{22}x_2+\dots+a_{2n}x_n=b_2\\
\dots\\
a_{n1}x_1+a_{n2}x_2+\dots+a_{nn}x_n=b_n\\
\end{cases}\Longrightarrow
\begin{cases}
x_1=\frac{1}{a_{11}}(-a_{12}x_2-\dots-a_{1n}x_n+b_1)\\
x_2=\frac{1}{a_{22}}(-a_{21}x_1-\dots-a_{2n}x_n+b_2)\\
\dots\\
x_1=\frac{1}{a_{nn}}(-a_{n2}x_1-\dots-a_{nn-1}x_{n-1}+b_n)\\
\end{cases}
\end{equation*}
In matrix form, 
\begin{equation*}
A=
\begin{pmatrix}
D&-U&-U\\
-L&D&-U\\
-L&-L&D
\end{pmatrix}
\end{equation*}
\begin{align*}
A\bl{x}=\bl{b}&\Leftrightarrow(D-L-U)\bl{x}=\bl{b}\\
&\Leftrightarrow D\bl{x}=(L+U)\bl{x}+\bl{b}\\
&\Leftrightarrow \bl{x}=\underbrace{D^{-1}(L+U)}_{T_j}\bl{x}+\underbrace{D^{-1}}_{\bl{c}_j}\bl{b}
\end{align*}.
\(T_j\) is Jacobi iterative matrix. \(\bl{x}^{(k)}=T_j\bl{x}^{(k-1)}+\bl{c}_j\)


\textbf{Gauss-Seidel iterative method}
\begin{align*}
&\bl{x}^{(k)}=D^{-1}(L\bl{x}^{(k)}+U\bl{x}^{(k-1)})+D^{-1}\bl{b}\\
\Leftrightarrow&(D-L)\bl{x}^{(k)}=U\bl{x}^{(k-1)}+\bl{b}\\
\Leftrightarrow&\bl{x}^{(k)}=\underbrace{(D-L)^{-1}U\bl{x^{(k-1)}}}_{T_g}
+\underbrace{(D-L)^{-1}\bl{b}}_{\bl{c}_g}
\end{align*}


\textbf{convergence of iterative methods}
\begin{theorem}
the following are equivalent:
\begin{enumerate}
\item A is a convergent matrix
\item $\lim\limits_{n\to\infty}||A^n|| = 0$ for some natural norm
\item $\lim\limits_{n\to\infty}||A^n||=0$ for all natural norms
\item $\rho(A)<1$
\item $\lim\limits_{n\to\infty}A^n\bl{x}=\bl{0}$ for every $\bl{x}$
\end{enumerate}
\end{theorem}

\(\bl{e}^{(k)}=\bl{x}^{(k)}-\bl{x}^*=(T\bl{x}^{(k-1)}+\bl{c})-(T\bl{x}^*+\bl{c})
   =T(\bl{x}^{(k-1)}-\bl{x}^*)=T\bl{e}^{(k-1)}\Rightarrow\bl{e}^{(k)}=T^k\bl{e}^{(0)}\).
\(||\bl{e}^{(k)}\le||T||\cdot||\bl{e}^{(k-1)}||\le\dots\le||T||^k\cdot||bl{e}^{(0)}||\)

\begin{theorem}
For any $\bl{x}^{(0)}\in R^n$, the sequence $\{\bl{x}^{(k)}\}_{k=0}^\infty$
defined by $\bl{x}^{(k)}=T\bl{x}^{(k-1)}+\bl{c}$ for each k, converges to the
unique solution of $\bl{x}=T\bl{x}+\bl{c}$ if and only if $\rho(T)<1$
\end{theorem}
\subsection{7.4 Error bounds and iterative refinement}
\label{sec:orge496d2f}
Assume that A is accurate and \(\bl{b}\) has the error \(\delta \bl{b}\),
then \(\bl{A}(\bl{x}+\delta \bl{x})=\bl{b}+\delta \bl{b}\)

\begin{theorem}
Suppose $\tilde{\bl{x}}$ is an approximation to the solution of $ \bl{Ax=b}$
A is nonsingular matrix. Then for any natural norm,
\begin{equation*}
||\bl{x-\tilde{x}}||\le||\bl{r}||\cdot||A^{-1}||
\end{equation*}
and if $ \bl{x\neq 0, b\neq 0}$,
\begin{equation*}
\frac{||\delta\bl{x}||}{||\bl{x}||}\le||\bl{A}
||\cdot||\bl{A}^{-1}||\cdot \frac{||\delta\bl{b}||}{||\bl{b}||}
\end{equation*}
\end{theorem}

\begin{proof}
$\bl{r=b-A\tilde{x}}=A\bl{x}-A\tilde{\bl{x}}$ and A is nonsingular. Hence 
$\bl{x-\tilde{x}}=A^{-1}\bl{r}$. Since $\frac{||A^{-1}\bl{r}||}{||\bl{r}||}\le||A^{-1}||$
, $||\bl{x-\tilde{x}}||=||A^{-1}\bl{x}||\le||A^-1||\cdot||\bl{r}||$. Also
$||\bl{b}||\le||A||\cdot||\bl{x}||$. So $1/||\bl{x}||\le||A||/||\bl{b}||$
\end{proof}

\begin{theorem}
If a matrix B satisfies $||B||<1$ for some natural norm, then
\begin{enumerate}
\item $I\pm B$ is nonsingular
\item $||(I\pm B)^{-1}||\le \frac{1}{1-||B||}$
\end{enumerate}
\end{theorem}

Assume \(\bl{b}\) is accurate, A has the error \(\delta A\), then
\((A+\delta A)(\bl{x}+\delta\bl{x})=\bl{b}\). Hence
\(\frac{||\delta\bl{x}||}{||\bl{x}||}\le \frac{||A^{-1}||\cdot||\delta
   A||}{1-||A^{-1}||\cdot||\delta A||}=\frac{||A||\cdot||A^{-1}||}{1
   -||A||\cdot||A^{-1}||\cdot \frac{||\delta A||}{||A||}}\)

\textbf{condition number K(A)} is \(||A||\cdot||A^{-1}||\)

\begin{theorem}
Suppose A is nonsingular and $||\delta A||\le \frac{1}{||A^{-1}||}$. The solution
$\bl{x}+\delta\bl{x}$ to $(A+\delta A)(\bl{x}+\delta\bl{x})$ approximates the solution
$\bl{x}$ of $A\bl{x}=\bl{b}$ with the error estimate
\begin{equation*}
\frac{||\delta\bl{x}||}{||\bl{x}||}\le \frac{K(A)}{1-K(A)||\delta A||/||A||}
(\frac{||\delta A||}{||A||}+ \frac{||\delta\bl{b}||}{||\bl{b}||})
\end{equation*}
\end{theorem}

note:
\begin{enumerate}
\item If A is symmetric, then \(K(A)_2= \frac{\max|\lambda|}{\min|\lambda|}\)
\item \(K(A)_p\ge1\) for all natural norm
\item \(K(\alpha A)_=K(A)\) for any \(\alpha\in R\)
\item \(K(A)_2=1\) if A is orthogonal
\item \(K(RA)_2=K(AR)_2=K(A)_2\) for all orthogonal matrix R\_
\end{enumerate}


\textbf{iterative refinement}:
\begin{theorem}
Suppose $\bl{x}^*$ is an approximation to the solution of $A\bl{x}=\bl{b}$, A is
nonsingular matrix and $\bl{r}=\bl{b}-A\bl{x}$. Then for any natural norm,
$||\bl{x-x^*}\le||\bl{r}||\cdot||A^{-1}||$, and if $\bl{x,b}\neq\bl{0}$
\begin{equation*}
\frac{||\bl{x}-\bl{x}^*||}{||\bl{x}||}\le K(A)\frac{||\bl{r}||}{||\bl{b}||}
\end{equation*}
\end{theorem}

\textbf{refinement}
\begin{enumerate}
\item \(A\bl{x}=\bl{b}\) => approximation \(\bl{x}_1\)
\item \(\bl{r}_1=\bl{b}-A\bl{x}_1\)
\item \(A\bl{d}_1=\bl{r}_1\) => \(\bl{d}_1\)
\item \(\bl{x}_2=\bl{x}_1+\bl{d}_1\)
\end{enumerate}
\section{chap9 Approximating Eigenvalues}
\label{sec:org5fed992}
\subsection{9.3 the power method}
\label{sec:org304eb40}
\textbf{the original method}
Assumptions: A is an \(n\times n\) matrix with eigenvalues satisfying
\(|\lambda_1|>|\lambda_2|\ge\dots\ge|\lambda_n|\ge 0\)

\begin{align*}
&\bl{x}^{(0)}=\displaystyle\sum_{j=1}^{n}\beta_j\bl{v}_j,\quad\beta_1\neq 0\\
&\bl{x}^{(1)}=A\bl{x}^{(0)}=\displaystyle\sum_{j=1}^n\beta_j\lambda_j\bl{v}_j\\
&\bl{x}^{(2)}=A\bl{x}^{(1)}=\displaystyle\sum_{j=1}^n\beta_j\lambda_j^2\bl{v}_j\\
&\dots\\
&\bl{x}^{(k)}\approx\lambda_1^k\beta_1\bl{v}_1, \quad \lambda_1\approx
\frac{\bl{x}^{(k)}_i}{\bl{x}^{(k-1)}_i}
\end{align*}

\textbf{Normalization}. Suppose \(||\bl{x}||_\infty=1\). Let
\(||\bl{x}^{(k)}||_\infty=|x_{p_k}^{(k)}|\).Then \(\bl{u}^{(k-1)}=
   \frac{\bl{x}^{(k-1)}}{|x_{p_{k-1}}^{(k-1)}|}\) and
\(\bl{x}^{(k)}=A\bl{u}^{(k-1)}\).
Then \(\bl{u}^{(k)}= \frac{\bl{x}^{(k)}}{|x_{p_k}^{(k)}|}\to \bl{v}_1\).
\(\lambda_1\approx
   \frac{\bl{x}_i^{(k)}}{\bl{u}_i^{(k-1)}}=\bl{x}_{p_{k-1}}^{(k)}\)

Note:
\begin{enumerate}
\item the method works for \textbf{multiple} eigenvalues
\(\lambda_1=\lambda_2=\dots=\lambda_r\)
\item the method fails to converge if \(\lambda_1=-\lambda_2\)
\item Aitken's \(\Delta^2\) can be used
\end{enumerate}


\textbf{Rate of convergence}. \(\bl{x}^{(k)}=A\bl{x}^{(k-1)}=\lambda_1^k
   \displaystyle\sum_{j=1}^n\beta_j(\frac{\lambda_j}{\lambda_1})^k\bl{v}_j\).
Make \(|\lambda_2/\lambda_1|\) as small as possible.
Assume \(\lambda_1>\lambda_2\ge\dots\ge\lambda_n, |\lambda_2|>|\lambda_n|\).
Let \(B=A-pI\), then \(|\lambda I-A|=|\lambda I-(B+pI)|=|(\lambda-p)I-B|\).
Hence \(\lambda_A-p=\lambda_B\). Since  \(\frac{|\lambda_2-p|}{|\lambda_1-p|}<
   \frac{|\lambda_2|}{|\lambda_1|}\) . The iteration is fast


\textbf{Inverse power method}. If A has
 \(|\lambda_1|\ge|\lambda_2|\ge\dots>|\lambda_n|\), then \(A^{-1}\) has
 \(|\frac{1}{\lambda_n}|>| \frac{1}{\lambda_{n-1}}|\ge\dots\ge|
    \frac{1}{\lambda_1}|\) 
\end{document}