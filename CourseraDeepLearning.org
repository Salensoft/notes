#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{pgfplots}
#+LATEX_HEADER: \usetikzlibrary{shapes,snakes,calc}
%#+header: :file (by-backend (html "tree.svg") (t 'nil))
%#+header: :imagemagick
#+header: :results (by-backend (pdf "latex") (t "raw"))
* week1
** logistic regression
   the output of y in supervised learning problem are either zero or 1
   + given $x\in\mathbb{R}^{n_x}$, want $\hat{y}=p(y=1\mid x)$,parameters $w\in\mathbb{R}^{n_x},b\in\mathbb{r}$
     output $\hat{y}=\sigma(w^tx+b)$,$\sigma(z)=\frac{1}{1+e^{-z}}$.
     training example:$\{(x^{(1)},y^{(1)}),\dots,(x^{(m)},y^{(m)})\}$, want$\hat{y}^{(i)}\approx y^{(i)}$
     + loss function ::
                       measure how good $\hat{y}$ is when the true label is y
                       $\boldsymbol{l}(\hat{y},y)=-(y\text{log}\hat{y}+(1-y)\text{log}(1-\hat{y}))$
                       if y = 1, $\boldsymbol{l}=-\text{log}\hat{y}$, want $\hat{y}$ large
                       if y = 0, $\boldsymbol{l}=-\text{log}(1-\hat{y})$,want $\hat{y}$ small
     + cost function :: entire training set
                       measures how well we're doing an entire training set
                       $j(w,b)=\frac{1}{m}\displaystyle\sum_{i=1}^m\boldsymbol{l}(\hat{y}^{(i)},y^{(i)})$
** gradient descent
   + want to find $w,b$ that minimize $j(w,b)$
   + $w:=w-\alpha\frac{\partial j(w,b)}{\partial w}$
** logistic regression gradient descent
   \begin{align*}
   &\frac{\partial\boldsymbol{l}(a,y)}{\partial a}=-\frac{y}{a}+\frac{1-y}{1-a}\\
   &\frac{\partial a}{\partial z}=\frac{-e^{-x}}{(1+e^{-x})^2}=a(1-a)\\
   &\frac{\partial\boldsymbol{l}(a,y)}{\partial z}=a(1-a)(-\frac{y}{a}+\frac{1-y}{1-a})=a-y
   \end{align*}
** vectorization
   + simd :: single instantion multiple data
* week2
** neural network representaion
   \begin{tikzpicture}
   [place/.style={circle,draw=blue!50,fill=blue!20,minimum size=5mm}]
   \node (x1) at (0,2.5) [label=above:{$a^{[0]}= x$}] {$x_1$};
   \node (x2) at (0,1.5) {$x_2$};
   \node (x3) at (0,0.5) [label=below:input layer] {$x_3$};
   \node (p1) at (2,0) [place,label=below:hidden layer] {};
   \node (p2) at (2,1) [place] {};
   \node (p3) at (2,2) [place] {};
   \node (p4) at (2,3) [place,label=above:{$a^{[1]}$}] {};
   \node (c) at (4,1.5) [place,label=below:output layer] {$a^{[2]}$};
   \node (y) at (6,1.5) {$\hat{y}$};
   \draw [->] (x1) -- (p1) -- (c) -- (y);
   \draw [->] (x1) -- (p2) -- (c);
   \draw [->] (x1) -- (p4) -- (c);
   \draw [->] (x2) -- (p1);
   \draw [->] (x1) -- (p3) -- (c);
   \draw [->] (x2) -- (p1);
   \draw [->] (x2) -- (p2);
   \draw [->] (x2) -- (p3);
   \draw [->] (x3) -- (p3);
   \draw [->] (x3) -- (p2);
   \draw [->] (x3) -- (p1);
   \draw [->] (x3) -- (p4);
   \draw [->] (x2) -- (p4);
   \end{tikzpicture}
   + 2 layer nn
   \begin{tikzpicture}
   \node [circle split, draw, rotate=90] (z) {\rotatebox{-90}{$z_1^{[1]}=w_1^{[1]t}x+b_1^{[1]}$}
   \nodepart{lower} \rotatebox{-90}{$a_1^{[1]}=\sigma(z_1^{[1]})$}};
   \end{tikzpicture}
   \begin{align*}
   &z_1^{[1]}=w_1^{[1]t}x+b_1^{[1]}\quad a_1^{[1]}=\sigma(z_1^{[1]})\\
   &z_2^{[1]}=w_2^{[1]t}x+b_2^{[1]}\quad a_2^{[1]}=\sigma(z_2^{[1]})\\
   &z_3^{[1]}=w_3^{[1]t}x+b_3^{[1]}\quad a_3^{[1]}=\sigma(z_3^{[1]})\\
   &z_4^{[1]}=w_4^{[1]t}x+b_4^{[1]}\quad a_4^{[1]}=\sigma(z_4^{[1]})\\
   &z^{[2]}=w^{[2]t}a^{[1]}+b^{[2]}\quad a^{[2]}=\sigma(z^{[2]})\\
   \end{align*}
   \begin{pmatrix}
   \mid & \mid & \cdots & \mid \\
   x^{(1)} & x^{(2)} & \cdots & x^{(n)}\\
   \mid & \mid & \cdots & \mid \\
   \end{pmatrix}
** Activation function
   + sigmoid function
   + hypobolic tangent function
     \begin{equation*}
     g(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}
     \end{equation*}
     \begin{equation*}
     g'(z)=1-(tanh(z))^2
     \end{equation*}
     \begin{tikzpicture}
     \begin{axis}[
     xmin=-2.5, xmax=2.5,
     ymin=-1.5, ymax=1.5,
     axis lines=center,
     axis on top=true,
     domain=-2.5:2.5,
     ylabel=$y$,
     xlabel=$x$,
     ]
     \addplot [mark=none,draw=red,ultra thick] {tanh(\x)};
     \node [right, red] at (axis cs: 1,0.7) {$y = \tanh x$};
     \draw [blue, dotted, thick] (axis cs:-2.5,-1)-- (axis cs:0,-1);
     \draw [blue, dotted, thick] (axis cs:+2.5,+1)-- (axis cs:0,+1);
     \end{axis}
     \end{tikzpicture}
   + rectified linear unit
     \begin{tikzpicture}
     \begin{axis}
     [xmin=-1.5, xmax=1.5,
     ymin=-1.5, ymax=1.5,
     axis lines=center,
     axis on top=true,
     domain=-1.5:1.5,
     ylabel=$a$,
     xlabel=$z$,]
     \draw [purple, thick] (axis cs:-1,0) -- (axis cs:0,0);
     \draw [purple, thick] (axis cs:0,0) -- (axis cs:1,1);
     \node [right, purle] at (axis cs: 0.3,0.7) {$a = max(0,z)$};
     \end{axis}
     \end{tikzpicture}
   + Leaky ReLU
     \begin{tikzpicture}
     \begin{axis}
     [xmin=-1.5, xmax=1.5,
     ymin=-1.5, ymax=1.5,
     axis lines=center,
     axis on top=true,
     domain=-1.5:1.5,
     ylabel=$a$,
     xlabel=$z$,]
     \draw [purple, thick] (axis cs:-1,-0.2) -- (axis cs:0,0);
     \draw [purple, thick] (axis cs:0,0) -- (axis cs:1,1);
     \node [right, purle] at (axis cs: 0.3,0.7) {$Leaky ReLU$};
     \end{axis}
     \end{tikzpicture}
   + sigmoid function: never use except for output
   + tanh is better
   + ReLU: commonly used
** Random initialization
   \begin{tikzpicture}
   \node (x1) at (0,2) {$x_1$};
   \node (x2) at (0,0) {$x_2$};
   \node (a1) at (2,2) [circle,draw] {$a_1^{[1]}$};
   \node (a2) at (2,0) [circle,draw] {$a_2^{[1]}$};
   \node (a12) at (4,1) [circle,draw] {$a_1^{[2]}$};
   \node (y) at (6,1) {$\hat{y}$};
   \draw [->] (x1) -- (a1) -- (a12) -- (y);
   \draw [->] (x1) -- (a2) -- (a12);
   \draw [->] (x2) -- (a1);
   \draw [->] (x2) -- (a2);
   \end{tikzpicture}
   if w is 0
   $a_1^{[1]}$ will be the same as $a_2^{[1]}$
* Week3
** Deep neural network notation
   \begin{tikzpicture}
   \node (x1) at (0, 6) [circle, draw] {$x_1$};
   \node (x2) at (0, 4) [circle, draw] {$x_2$};
   \node (x3) at (0, 2) [circle, draw] {$x_3$};
   \node (n11) at (2, 8) [circle, draw, label=above:layer 1] {};
   \node (n12) at (2, 6) [circle, draw] {};
   \node (n13) at (2, 4) [circle, draw] {};
   \node (n14) at (2, 2) [circle, draw] {};
   \node (n15) at (2, 0) [circle, draw] {};
   \node (n21) at (4, 8) [circle, draw, label=above:layer 2] {};
   \node (n22) at (4, 6) [circle, draw] {};
   \node (n23) at (4, 4) [circle, draw] {};
   \node (n24) at (4, 2) [circle, draw] {};
   \node (n25) at (4, 0) [circle, draw] {};
   \node (n31) at (6, 6) [circle, draw] {};
   \node (n32) at (6, 4) [circle, draw] {};
   \node (n33) at (6, 2) [circle, draw] {};
   \node (n41) at (8, 4) [circle, draw] {};
   \node (y) at (10, 4) [circle, draw] {$\hat{y}=a^{[l]}$};
   \draw [->] (x1) -- (n11);
   \draw [->] (x1) -- (n12);
   \draw [->] (x1) -- (n13);
   \draw [->] (x1) -- (n14);
   \draw [->] (x1) -- (n15);
   \draw [->] (x2) -- (n11);
   \draw [->] (x2) -- (n12);
   \draw [->] (x2) -- (n13);
   \draw [->] (x2) -- (n14);
   \draw [->] (x2) -- (n15);
   \draw [->] (x3) -- (n11);
   \draw [->] (x3) -- (n12);
   \draw [->] (x3) -- (n13);
   \draw [->] (x3) -- (n14);
   \draw [->] (x3) -- (n15);
   \draw [->] (n11) -- (n21);
   \draw [->] (n11) -- (n22);
   \draw [->] (n11) -- (n23);
   \draw [->] (n11) -- (n24);
   \draw [->] (n11) -- (n25);
   \draw [->] (n12) -- (n21);
   \draw [->] (n12) -- (n22);
   \draw [->] (n12) -- (n23);
   \draw [->] (n12) -- (n24);
   \draw [->] (n12) -- (n25);
   \draw [->] (n13) -- (n21);
   \draw [->] (n13) -- (n22);
   \draw [->] (n13) -- (n23);
   \draw [->] (n13) -- (n24);
   \draw [->] (n13) -- (n25);
   \draw [->] (n14) -- (n21);
   \draw [->] (n14) -- (n22);
   \draw [->] (n14) -- (n23);
   \draw [->] (n14) -- (n24);
   \draw [->] (n14) -- (n25);
   \draw [->] (n15) -- (n21);
   \draw [->] (n15) -- (n22);
   \draw [->] (n15) -- (n23);
   \draw [->] (n15) -- (n24);
   \draw [->] (n15) -- (n25);
   \draw [->] (n21) -- (n31);
   \draw [->] (n21) -- (n32);
   \draw [->] (n21) -- (n33);
   \draw [->] (n22) -- (n31);
   \draw [->] (n22) -- (n32);
   \draw [->] (n22) -- (n33);
   \draw [->] (n23) -- (n31);
   \draw [->] (n23) -- (n32);
   \draw [->] (n23) -- (n33);
   \draw [->] (n24) -- (n31);
   \draw [->] (n24) -- (n32);
   \draw [->] (n24) -- (n33);
   \draw [->] (n25) -- (n31);
   \draw [->] (n25) -- (n32);
   \draw [->] (n25) -- (n33);
   \draw [->] (n31) -- (n41);
   \draw [->] (n32) -- (n41);
   \draw [->] (n33) -- (n41);
   \draw [->] (n41) -- (y);
   \end{tikzpicture}
   $l$ is the $l$th layer
   $n^{[l]}=#units$ in layer $l$
   $a^{[l]}=g^{[l]}(z^{[l]})$ is activations in l
** Circuit theory and deep learning
   Informally: There are functions you can compute with a "small"
   L-layer deep neural network that shallower networks require exponentially
   more hidden units to compute
* Week4
** Bias and variance

   |-----------------+---------------+-----------+------------------------+------|
   | Train set error |            1% |       15% |                    15% |   1% |
   | Dev set error   |           11% |       16% |                    30% | 0.5% |
   |                 | high variance | high bias | high bias and variance |  low |
   |                 |               |           |                        |      |
   |-----------------+---------------+-----------+------------------------+------|
** Regularization
   + L2 regulation--logistic regression ::
        $\mathcal{J}(w,b)=\frac{1}{m}\displaystyle\sum_{i=1}^m\mathcal{L}(\hat{y}^{(i)}
        ,y^{(i)})+\frac{\lambda}{2m}||w||_2^2$
        $\lambda$ is regulazation parameter
   + neural network ::
     + $\mathcal{J}(w^{[1]},b^{[1]},\dots,w^{[L]},b^{[L]})=\frac{1}{m}\displaystyle\sum_{i=1}^m
       \mathcal{L}(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}||w^{[l]}||_F^2$
     + Frobenius norm ::
       + $||w^{[l]}||^2=\displaystyle\sum_{i=1}^{n^{[l-1]}}\sum_{j=1}^{n^{[l]}}
         (w_{ij}^{[l]})^2$
       + $dw^{[l]}=\dots + \frac{\lambda}{m}w^{[l]}$
   + Why regulazation ::
